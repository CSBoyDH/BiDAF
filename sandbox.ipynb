{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from process_data import save_pickle, load_pickle, load_task, load_glove_weights\n",
    "from process_data import to_var, make_word_vector, make_char_vector\n",
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: 1.1\n",
      "load_task: 0 / 442\n",
      "dataset version: 1.1\n",
      "load_task: 0 / 48\n",
      "----\n",
      "n_train 188\n",
      "ctx_maxlen 2060\n",
      "vocab_size_w: 4086\n",
      "vocab_size_c: 91\n",
      "ctx_sent_maxlen: 375\n",
      "query_sent_maxlen: 34\n",
      "ctx_word_maxlen: 22\n",
      "query_word_maxlen: 16\n"
     ]
    }
   ],
   "source": [
    "train_data, train_ctx_maxlen = load_task('./dataset/train-v1.1.json')\n",
    "train_data = train_data[:int(len(train_data)*0.7)]\n",
    "dev_data, dev_ctx_maxlen = load_task('./dataset/dev-v1.1.json')\n",
    "data = train_data + dev_data\n",
    "ctx_maxlen = max(train_ctx_maxlen, dev_ctx_maxlen)\n",
    "# save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "# save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "vocab_w, vocab_c = set(), set()\n",
    "for ctx_w, ctx_c, q_id, q_w, q_c, answer, _, _ in data:\n",
    "    vocab_w |= set(ctx_w + q_w + answer)\n",
    "    flatten_c = [c for chars in ctx_c for c in chars]\n",
    "    flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "    vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "\n",
    "vocab_w = list(sorted(vocab_w))\n",
    "vocab_c = list(sorted(vocab_c))\n",
    "\n",
    "w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "# save_pickle(vocab, 'pickle/vocab.pickle')\n",
    "# save_pickle(w2i, 'pickle/w2i.pickle')\n",
    "# save_pickle(i2w, 'pickle/i2w.pickle')\n",
    "# train_data = load_pickle('pickle/train_data.pickle')\n",
    "# vocab = load_pickle('pickle/vocab.pickle')\n",
    "# w2i = load_pickle('pickle/w2i.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _, _, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "# print('n_dev', len(dev_data))\n",
    "print('ctx_maxlen', ctx_maxlen)\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (4086, 100)\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "glove_embd_w = torch.from_numpy(load_glove_weights('./dataset', embd_size, vocab_size_w, w2i_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "batch 0 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 1 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 2 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 3 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 4 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 5 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 6 / 998\n",
      "build_context\n",
      "------char cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonki/anaconda3/lib/python3.6/site-packages/torch/autograd/_functions/basic_ops.py:48: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return a.mul(b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_context\n",
      "------char cuda\n",
      "batch 7 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 8 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 9 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 10 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 11 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 12 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 13 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 14 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 15 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 16 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 17 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 18 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 19 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 20 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 21 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 22 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 23 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 24 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 25 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 26 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 27 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 28 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 29 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 30 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 31 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 32 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 33 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 34 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 35 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 36 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 37 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 38 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 39 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 40 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 41 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 42 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 43 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 44 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 45 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 46 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 47 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 48 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 49 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 50 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 51 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 52 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 53 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 54 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 55 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 56 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 57 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 58 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 59 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 60 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 61 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 62 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 63 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 64 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 65 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 66 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 67 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 68 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 69 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 70 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 71 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 72 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 73 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 74 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 75 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 76 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 77 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 78 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 79 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 80 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 81 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 82 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 83 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 84 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 85 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 86 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 87 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 88 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 89 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 90 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 91 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 92 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 93 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 94 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 95 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 96 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 97 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 98 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 99 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 100 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 101 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 102 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 103 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 104 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 105 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 106 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 107 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 108 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 109 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 110 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 111 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 112 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 113 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 114 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 115 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 116 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 117 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 118 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 119 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 120 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 121 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 122 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 123 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 124 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 125 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 126 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 127 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 128 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 129 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 130 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 131 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 132 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 133 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 134 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 135 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 136 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 137 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 138 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 139 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 140 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 141 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 142 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 143 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 144 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 145 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 146 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 147 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 148 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 149 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 150 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 151 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 152 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 153 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 154 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 155 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 156 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 157 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 158 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 159 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 160 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 161 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 162 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 163 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 164 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 165 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 166 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 167 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 168 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 169 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 170 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 171 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 172 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 173 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 174 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 175 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 176 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 177 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 178 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 179 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 180 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 181 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 182 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 183 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 184 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 185 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 186 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 187 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n",
      "batch 188 / 998\n",
      "build_context\n",
      "------char cuda\n",
      "build_context\n",
      "------char cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: mismatch between the batch size of input (1) and that of target (3) at /pytorch/torch/lib/THCUNN/generic/ClassNLLCriterion.cu:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a1fa6913bde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# print(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finish train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a1fa6913bde3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, n_epoch, batch_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mq_word_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2i_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_sent_maxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_char_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_word_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_char_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_word_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mloss_p1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_beg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mloss_p2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         return F.nll_loss(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 132\u001b[0;31m                           self.ignore_index)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: mismatch between the batch size of input (1) and that of target (3) at /pytorch/torch/lib/THCUNN/generic/ClassNLLCriterion.cu:41"
     ]
    }
   ],
   "source": [
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "\n",
    "args = {\n",
    "    'embd_size': embd_size,\n",
    "    'vocab_size_c': vocab_size_c,\n",
    "    'vocab_size_w': vocab_size_w,\n",
    "    'pre_embd_w': glove_embd_w, # word embedding\n",
    "    'filters': [[1, 5]], # char embedding\n",
    "    'out_chs': 100, # char embedding\n",
    "    'ans_size': ctx_maxlen\n",
    "}\n",
    "args = Config(**args)\n",
    "\n",
    "\n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        self.embd_size = args.embd_size\n",
    "        self.ans_size = args.ans_size\n",
    "        self.char_embd_net = CharEmbedding(args)\n",
    "        self.word_embd_net = WordEmbedding(args)\n",
    "        self.highway_net = Highway(args.embd_size*2)# TODO share is ok?\n",
    "        self.ctx_embd_layer = nn.GRU(args.embd_size*2, args.embd_size*2, bidirectional=True)\n",
    "        self.W = nn.Parameter(torch.rand(3*2*2* args.embd_size, 1).type(torch.FloatTensor), requires_grad=True)\n",
    "#         self.beta = nn.Parameter(torch.rand(8*2*2* args.embd_size).type(torch.FloatTensor).view(1, -1), requires_grad=True)\n",
    "        self.modeling_layer = nn.GRU(args.embd_size*2*8, args.embd_size*2, bidirectional=True)\n",
    "        self.p1_layer = nn.Linear(args.embd_size*2*10, args.ans_size)\n",
    "        self.p2_lstm_layer = nn.GRU(args.embd_size*2*2, args.embd_size*2*2, bidirectional=True)\n",
    "        self.p2_layer = nn.Linear(args.embd_size*2*12, args.ans_size)\n",
    "        \n",
    "    def build_contextual_embd(self, x_c, x_w):\n",
    "        # 1. Caracter Embedding Layer\n",
    "        self.char_embd_net = self.char_embd_net.cuda()\n",
    "        char_embd = self.char_embd_net(x_c) # (N, seq_len, embd_size)\n",
    "        print('build_context')\n",
    "        if torch.cuda.is_available():\n",
    "            print('------char cuda')\n",
    "            char_embd = char_embd.cuda()\n",
    "        else:\n",
    "            print('not cuda ')\n",
    "        # 2. Word Embedding Layer\n",
    "        word_embd = self.word_embd_net(x_w) # (N, seq_len, embd_size)\n",
    "        if torch.cuda.is_available():\n",
    "            word_embd = word_embd.cuda()\n",
    "        # Highway Networks of 1. and 2.\n",
    "        embd = torch.cat((char_embd, word_embd), 2) # (N, seq_len, embd_size*2)\n",
    "        embd = self.highway_net(embd)\n",
    "        \n",
    "        # 3. Contextual  Embedding Layer\n",
    "        ctx_embd_out, ctx_embd_h = self.ctx_embd_layer(embd)\n",
    "        return ctx_embd_out\n",
    "        \n",
    "    def forward(self, ctx_c, ctx_w, query_c, query_w):\n",
    "        batch_size = ctx_c.size(0)\n",
    "        \n",
    "        # 1. Caracter Embedding Layer \n",
    "        # 2. Word Embedding Layer\n",
    "        # 3. Contextual  Embedding Layer\n",
    "        embd_context = self.build_contextual_embd(ctx_c, ctx_w) # (N, T, 2d)\n",
    "        ctx_len = embd_context.size(1)\n",
    "        embd_query   = self.build_contextual_embd(query_c, query_w) # (N, J, 2d)\n",
    "        query_len = embd_query.size(1)\n",
    "        \n",
    "        # 4. Attention Flow Layer\n",
    "        # Context2Query\n",
    "        shape = (batch_size, ctx_len, query_len, self.embd_size*2*2) # (N, T, J, 2d)\n",
    "        embd_context_ex = embd_context.unsqueeze(2) # (N, T, 1, 2d)\n",
    "        embd_context_ex = embd_context_ex.expand(shape)\n",
    "        embd_query_ex = embd_query.unsqueeze(1) # (N, 1, J, 2d)\n",
    "        embd_query_ex = embd_query_ex.expand(shape)\n",
    "        a_elmwise_mul_b = torch.mul(embd_context_ex, embd_query_ex) # (N, T, J, 2d)\n",
    "        cat_data = torch.cat((embd_context_ex, embd_query_ex, a_elmwise_mul_b), 3) # (N, T, J, 6d)\n",
    "        cat_data = cat_data.view(batch_size, -1, 6*2*self.embd_size)\n",
    "        S = torch.bmm(cat_data, self.W.unsqueeze(0).expand(batch_size, 6*2*self.embd_size, 1))\n",
    "        S = S.view(batch_size, ctx_len, query_len)\n",
    "        \n",
    "        c2q = torch.bmm(S, embd_query) # (N, T, 2d)\n",
    "        # Query2Context\n",
    "        tmp_b = torch.max(S, 2)[0]\n",
    "        b = torch.stack([F.softmax(tmp_b[i]) for i in range(batch_size)], 0) # (N, T)\n",
    "        q2c = torch.bmm(b.unsqueeze(1), embd_context).squeeze() # (N, 2d)\n",
    "        q2c = q2c.unsqueeze(1) # (N, 1, 2d)\n",
    "        q2c = q2c.repeat(1, ctx_len, 1) # (N, T, 2d)\n",
    "        \n",
    "        G = torch.cat((embd_context, c2q, embd_context.mul(c2q), embd_context.mul(q2c)), 2) # (N, T, 8d)\n",
    "        \n",
    "        # 5. Modeling Layer\n",
    "        M, _ = self.modeling_layer(G) # M: (N, T, 2d)\n",
    "        \n",
    "        # 5. Output Layer\n",
    "        G_M = torch.cat((G, M), 2) # (N, T, 10d)\n",
    "        G_M = G_M.sum(1) #(N, 10d)\n",
    "        p1 = F.softmax(self.p1_layer(G_M)) # (N, T)\n",
    "        \n",
    "        M2, _ = self.p2_lstm_layer(M) # (N, T, 4d)\n",
    "        G_M2 = torch.cat((G, M2), 2) # (N, T, 12d)\n",
    "        G_M2 = G_M2.sum(1) # (N, 12d)(N, T)\n",
    "        p2 = F.softmax(self.p2_layer(G_M2)) # (N, T)\n",
    "        \n",
    "        return p1, p2\n",
    "        \n",
    "def train(model, optimizer, n_epoch=10, batch_size=1):\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # TODO shuffle, last elms\n",
    "            print('batch', i, '/', len(data))\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            c = [d[0] for d in batch_data]\n",
    "            cc = [d[1] for d in batch_data]\n",
    "            q = [d[3] for d in batch_data]\n",
    "            qc = [d[4] for d in batch_data]\n",
    "            a_beg = to_var(torch.LongTensor([d[6] for d in batch_data]).squeeze())\n",
    "            a_end = to_var(torch.LongTensor([d[7] for d in batch_data]).squeeze())\n",
    "            c_char_var = make_char_vector(cc, w2i_c, ctx_sent_maxlen, ctx_word_maxlen)\n",
    "            c_word_var = make_word_vector(c, w2i_w, ctx_sent_maxlen)\n",
    "            q_char_var = make_char_vector(qc, w2i_c, query_sent_maxlen, query_word_maxlen)\n",
    "            q_word_var = make_word_vector(q, w2i_w, query_sent_maxlen)\n",
    "            p1, p2 = model(c_char_var, c_word_var, q_char_var, q_word_var)\n",
    "            loss_p1 = nn.NLLLoss()(p1, a_beg)\n",
    "            loss_p2 = nn.NLLLoss()(p2, a_end)\n",
    "            model.zero_grad()\n",
    "#             print('loss.backward()')\n",
    "            (loss_p1+loss_p2).backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             break\n",
    "model = AttentionNet(args)\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# print(model)\n",
    "optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=0.5)\n",
    "train(model, optimizer)\n",
    "print('finish train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 16\n",
    "hid_dim = 50\n",
    "a_seq_len = 10\n",
    "b_seq_len = 20\n",
    "a = torch.randn(N, a_seq_len, hid_dim)\n",
    "b = torch.randn(N, b_seq_len, hid_dim)\n",
    "shape = (N, a_seq_len, b_seq_len, hid_dim)\n",
    "\n",
    "result = torch.zeros(shape)\n",
    "\n",
    "a_dash = a.unsqueeze(2) # (N, a_len, 1,     hid_dim)\n",
    "b_dash = b.unsqueeze(1) # (N, 1,     b_len, hid_dim)\n",
    "a_dash = a_dash.expand(shape)\n",
    "b_dash = b_dash.expand(shape)\n",
    "mul = a_dash * b_dash\n",
    "\n",
    "print(a_dash.size(), b_dash.size(), mul.size())\n",
    "print(torch.cat((a_dash, b_dash, mul), 3).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "x_len = 3\n",
    "y_len = 5\n",
    "hid_dim = 8\n",
    "data = torch.randn(N, x_len, y_len, hid_dim)\n",
    "W = torch.randn(hid_dim)\n",
    "print(torch.mul(data, W).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hid_dim = 54\n",
    "tdata = torch.Tensor([[\n",
    "    [1] * hid_dim,\n",
    "    [2] * hid_dim,\n",
    "    [3] * hid_dim \n",
    "],\n",
    "[\n",
    "    [1] * hid_dim,\n",
    "    [2] * hid_dim,\n",
    "    [3] * hid_dim \n",
    "]])\n",
    "print('data', tdata.size())\n",
    "tW = torch.randn(hid_dim).view(-1, 1) # assume trainable parameters via nn.Parameter\n",
    "print('W', tW.size())\n",
    "\n",
    "print(tdata.size(), tW.unsqueeze(0).size())\n",
    "print(torch.bmm(tdata, tW.unsqueeze(0)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hid_dim = 32\n",
    "data = torch.randn(10, 6, hid_dim)\n",
    "# data = tdata.view(10, 2*3, hid_dim)\n",
    "W = torch.randn(hid_dim, 1) # assume trainable parameters via nn.Parameter\n",
    "print(W.size())\n",
    "W = W.unsqueeze(0).expand(10, hid_dim, 1)\n",
    "print(W.size())\n",
    "result = torch.bmm(data, W).squeeze() # error, want (N, 6)\n",
    "print(result.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hid_dim = 32\n",
    "data = torch.randn(10, 2, 3, hid_dim)\n",
    "data = tdata.view(10, 2*3, hid_dim)\n",
    "W = torch.randn(hid_dim, 1) # assume trainable parameters via nn.Parameter\n",
    "print(W.size())\n",
    "W = W.unsqueeze(0).expand(10, hid_dim, 1)\n",
    "print(W.size())\n",
    "result = torch.bmm(data, W).squeeze() # error, want (N, 6)\n",
    "result = result.view(10, 2, 3)\n",
    "print(result.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
