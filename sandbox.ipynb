{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from process_data import save_pickle, load_pickle, load_glove_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_task(dataset_path):\n",
    "    ret_data = []\n",
    "    with open(dataset_path) as f:\n",
    "        data = json.load(f)\n",
    "        ver = data['version']\n",
    "        print('dataset version:', ver)\n",
    "        data = data['data']\n",
    "        for i, d in enumerate(data):\n",
    "            if i % 100 == 0: print('load_task:', i, '/', len(data))\n",
    "            # print('load', d['title'], i, '/', len(data))\n",
    "            for p in d['paragraphs']:\n",
    "                c = word_tokenize(p['context'])\n",
    "                cc = [list(w) for w in c]\n",
    "                q, a = [], []\n",
    "                for qa in p['qas']:\n",
    "                    q = word_tokenize(qa['question'])\n",
    "                    qc = [list(w) for w in q]\n",
    "                    a = [ans['text'] for ans in qa['answers']]\n",
    "                    ret_data.append((c, cc, qa['id'], q, qc, a)) # TODO context redandancy\n",
    "#                 break\n",
    "            break\n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: 1.1\n",
      "load_task: 0 / 442\n",
      "----\n",
      "n_train 269\n",
      "n_dev 30\n",
      "vocab_size_w: 2845\n",
      "vocab_size_c: 89\n",
      "ctx_sent_maxlen: 333\n",
      "query_sent_maxlen: 25\n",
      "ctx_word_maxlen: 22\n",
      "query_word_maxlen: 14\n"
     ]
    }
   ],
   "source": [
    "train_data = load_task('./dataset/train-v1.1.json')\n",
    "# dev_data = load_task('./dataset/dev-v1.1.json')\n",
    "data = train_data # + dev_data\n",
    "# save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "# save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "vocab_w, vocab_c = set(), set()\n",
    "for ctx_w, ctx_c, q_id, q_w, q_c, answer in train_data+dev_data:\n",
    "    vocab_w |= set(ctx_w + q_w + answer)\n",
    "    flatten_c = [c for chars in ctx_c for c in chars]\n",
    "    flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "    vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "\n",
    "vocab_w = list(sorted(vocab_w))\n",
    "vocab_c = list(sorted(vocab_c))\n",
    "\n",
    "w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "# save_pickle(vocab, 'pickle/vocab.pickle')\n",
    "# save_pickle(w2i, 'pickle/w2i.pickle')\n",
    "# save_pickle(i2w, 'pickle/i2w.pickle')\n",
    "# train_data = load_pickle('pickle/train_data.pickle')\n",
    "# vocab = load_pickle('pickle/vocab.pickle')\n",
    "# w2i = load_pickle('pickle/w2i.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "print('n_dev', len(dev_data))\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T', 'o'],\n",
       " ['w', 'h', 'o', 'm'],\n",
       " ['d', 'i', 'd'],\n",
       " ['t', 'h', 'e'],\n",
       " ['V', 'i', 'r', 'g', 'i', 'n'],\n",
       " ['M', 'a', 'r', 'y'],\n",
       " ['a', 'l', 'l', 'e', 'g', 'e', 'd', 'l', 'y'],\n",
       " ['a', 'p', 'p', 'e', 'a', 'r'],\n",
       " ['i', 'n'],\n",
       " ['1', '8', '5', '8'],\n",
       " ['i', 'n'],\n",
       " ['L', 'o', 'u', 'r', 'd', 'e', 's'],\n",
       " ['F', 'r', 'a', 'n', 'c', 'e'],\n",
       " ['?']]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent1 = list('i have a cat')\n",
    "# sent2 = list('i had a aieu')\n",
    "# data = [sent1, sent2]\n",
    "# vocab = set(sent1+sent2)\n",
    "# vocab_size = len(vocab)\n",
    "# print(data)\n",
    "# print('vocab_size', vocab_size)\n",
    "# w2i = {w:i for i, w in enumerate(vocab)}\n",
    "# i2w = {i:w for i, w in enumerate(vocab)}\n",
    "\n",
    "# max_word_len = max([len(word) for word in data])\n",
    "# print('max_word_len', max_word_len)\n",
    "data[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_var.size()= torch.Size([1, 25, 14])\n",
      "CharEmbedding (\n",
      "  (embedding): Embedding(89, 100)\n",
      "  (conv): ModuleList (\n",
      "    (0): Conv2d(1, 100, kernel_size=(1, 5), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout (p = 0.5)\n",
      "  (fc1): Linear (100 -> 1)\n",
      ")\n",
      "x torch.Size([1, 25, 14])\n",
      "out torch.Size([1, 25, 100])\n"
     ]
    }
   ],
   "source": [
    "# In : (N, sentence_len, word_len, vocab_size_c)\n",
    "# Out: (N, sentence_len, embd_size)\n",
    "# hoge = []\n",
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, out_chs, filters):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.embd_size = embd_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
    "        # nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1, out_chs, (f[0], f[1])) for f in filters])\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(out_chs*len(filters), 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('x', x.size()) # (N, seq_len, word_len)\n",
    "        bs = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        word_len = x.size(2)\n",
    "        embd = Variable(torch.zeros(bs, seq_len, self.embd_size))\n",
    "        for i, elm in enumerate(x): # every sample\n",
    "            for j, chars in enumerate(elm): # every sentence. [ [‘w’, ‘h’, ‘o’, 0], [‘i’, ‘s’, 0, 0], [‘t’, ‘h’, ‘i’, ‘s’] ]\n",
    "                chars_embd = self.embedding(chars.unsqueeze(0)) # (N, word_len, embd_size) [‘w’,‘h’,‘o’,0]\n",
    "                chars_embd = torch.sum(chars_embd, 1) # (N, embd_size). sum each char's embedding\n",
    "                embd[i,j] = chars_embd[0] # set char_embd as word-like embedding\n",
    "\n",
    "        x = embd # (N, seq_len, embd_dim)\n",
    "        x = embd.unsqueeze(1) # (N, Cin, seq_len, embd_dim), insert Channnel-In dim\n",
    "        # Conv2d\n",
    "        #    Input : (N,Cin, Hin, Win )\n",
    "        #    Output: (N,Cout,Hout,Wout) \n",
    "        x = [F.relu(conv(x)) for conv in self.conv] # (N, Cout, seq_len, embd_dim-filter_w+1). stride == 1\n",
    "        \n",
    "        # [(N,Cout,Hout,Wout) -> [(N,Cout,Hout*Wout)] * len(filter_heights)\n",
    "        # [(N, seq_len, embd_dim-filter_w+1, Cout)] * len(filter_heights)\n",
    "        x = [xx.view((xx.size(0), xx.size(2), xx.size(3), xx.size(1))) for xx in x]\n",
    "        \n",
    "        # maxpool like\n",
    "        # [(N, seq_len, Cout)] * len(filter_heights)\n",
    "        x = [torch.sum(xx, 2) for xx in x]\n",
    "        out = torch.cat(x, 1) # (N, seq_len, Cout)\n",
    "        return out\n",
    "\n",
    "embd_size = 100\n",
    "n_out_ch = 100\n",
    "filters = [[1, 5]]\n",
    "tmp_data = data[0][4]\n",
    "max_len = max([len(chars) for chars in tmp_data])\n",
    "tmp_var = torch.zeros(1, query_sent_maxlen, query_word_maxlen).type(torch.LongTensor)\n",
    "print('tmp_var.size()=', tmp_var.size())\n",
    "for i, chars in enumerate(tmp_data):\n",
    "    for j, ch in enumerate(chars):\n",
    "        tmp_var[0][i][j] = w2i_c[ch]\n",
    "char_embd_net = CharEmbedding(vocab_size_c, embd_size, n_out_ch, filters)\n",
    "print(char_embd_net)\n",
    "out = char_embd_net(Variable(tmp_var))\n",
    "print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (6, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "glove_embd_w = torch.from_numpy(load_glove_weights('./', embd_size, vocab_size, w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'have', 'a', 'cat'], ['i', 'had', 'a', 'aieu']]\n",
      "vocab_size 6\n",
      "max_word_len 4\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'i have a cat'.split(' ')\n",
    "sent2 = 'i had a aieu'.split(' ')\n",
    "data = [sent1, sent2]\n",
    "vocab = set(sent1+sent2)\n",
    "vocab_size = len(vocab)\n",
    "print(data)\n",
    "print('vocab_size', vocab_size)\n",
    "w2i = {w:i for i, w in enumerate(vocab)}\n",
    "i2w = {i:w for i, w in enumerate(vocab)}\n",
    "\n",
    "max_word_len = max([len(word) for word in data])\n",
    "print('max_word_len', max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out torch.Size([1, 4, 100])\n",
      "torch.Size([1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "# In : (N, sentence_len, vocab_size_w)\n",
    "# Out: (N, sentence_len, embd_size)\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, is_train_embd=False, pre_embd_w=None):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
    "        if pre_embd_w is not None:\n",
    "            self.embedding.weight = nn.Parameter(pre_embd_w, requires_grad=is_train_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out = F.relu(x)\n",
    "        print('out', out.size())\n",
    "        return out\n",
    "\n",
    "word_embd_net = WordEmbedding(vocab_size, embd_size, False, glove_embd_w)\n",
    "word_var = Variable(torch.LongTensor([[w2i[w] for w in data[0]]]))\n",
    "out = word_embd_net(word_var)\n",
    "print(out.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 14, 100])\n"
     ]
    }
   ],
   "source": [
    "print(type(hoge[0]))\n",
    "print(hoge[0].size())\n",
    "# print(hoge[1])\n",
    "# print(hoge[0]+hoge[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -0.8029  0.0716  0.8087 -1.8352\n",
       " -1.5496  0.7203  0.9974 -0.5096\n",
       "  0.1004 -0.0163 -1.1995  0.1710\n",
       "\n",
       "(1 ,.,.) = \n",
       "  1.7253  0.6494  0.9882 -1.8014\n",
       " -0.1006  1.2877  0.2056 -1.6416\n",
       " -0.7419  0.4738  1.6681 -0.5003\n",
       "[torch.FloatTensor of size 2x3x4]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-2.2521  0.7757  0.6066 -2.1738\n",
       " 0.8828  2.4109  2.8619 -3.9433\n",
       "[torch.FloatTensor of size 2x4]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
