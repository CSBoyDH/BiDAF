{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from process_data import save_pickle, load_pickle, load_task, load_glove_weights\n",
    "from process_data import to_var, make_word_vector, make_char_vector\n",
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "from layers.attention_net import AttentionNet\n",
    "from config import Config\n",
    "\n",
    "import sys\n",
    "sys.argv = ['a.py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pickle/train_data.pickle\n",
      "load pickle/dev_data.pickle\n",
      "load pickle/vocab_w.pickle\n",
      "load pickle/vocab_c.pickle\n",
      "load pickle/w2i_w.pickle\n",
      "load pickle/i2w_w.pickle\n",
      "load pickle/w2i_c.pickle\n",
      "load pickle/i2w_c.pickle\n",
      "----\n",
      "n_train 87599\n",
      "ctx_maxlen 4063\n",
      "vocab_size_w: 186069\n",
      "vocab_size_c: 1419\n",
      "ctx_sent_maxlen: 766\n",
      "query_sent_maxlen: 60\n",
      "ctx_word_maxlen: 37\n",
      "query_word_maxlen: 30\n",
      "load ./pickle/glove_embd_w.pickle\n",
      "Namespace(ans_size=4063, batch_size=8, c_embd_size=8, filters=[[1, 5]], lr=0.5, manualSeed=None, ngpu=1, out_chs=100, pre_embd_w=\n",
      " 0.3847  0.4935  0.4910  ...   0.0263  0.3905  0.5222\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "          ...             â‹±             ...          \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 186069x100]\n",
      ", use_pickle=1, vocab_size_c=1419, vocab_size_w=186069, w_embd_size=100)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='input batch size')\n",
    "parser.add_argument('--lr', type=float, default=0.5, help='learning rate, default=0.5')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--w_embd_size', type=int, default=100, help='word embedding size')\n",
    "parser.add_argument('--c_embd_size', type=int, default=8, help='character embedding size')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('--use_pickle', type=int, default=1, help='load dataset from pickles')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.use_pickle == 1:\n",
    "    train_data = load_pickle('pickle/train_data.pickle')\n",
    "    dev_data = load_pickle('pickle/dev_data.pickle')\n",
    "    data = train_data + dev_data\n",
    "    ctx_maxlen = 4063 #TODO\n",
    "\n",
    "    vocab_w = load_pickle('pickle/vocab_w.pickle')\n",
    "    vocab_c = load_pickle('pickle/vocab_c.pickle')\n",
    "    w2i_w = load_pickle('pickle/w2i_w.pickle')\n",
    "    i2w_w = load_pickle('pickle/i2w_w.pickle')\n",
    "    w2i_c = load_pickle('pickle/w2i_c.pickle')\n",
    "    i2w_c = load_pickle('pickle/i2w_c.pickle')\n",
    "else:\n",
    "    train_data, train_ctx_maxlen = load_task('./dataset/train-v1.1.json')\n",
    "    dev_data, dev_ctx_maxlen = load_task('./dataset/dev-v1.1.json')\n",
    "    data = train_data + dev_data\n",
    "    ctx_maxlen = max(train_ctx_maxlen, dev_ctx_maxlen)\n",
    "    save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "    save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "    vocab_w, vocab_c = set(), set()\n",
    "    for ctx_w, ctx_c, q_id, q_w, q_c, answer, _, _ in data:\n",
    "        vocab_w |= set(ctx_w + q_w + answer)\n",
    "        flatten_c = [c for chars in ctx_c for c in chars]\n",
    "        flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "        vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "    vocab_w = list(sorted(vocab_w))\n",
    "    vocab_c = list(sorted(vocab_c))\n",
    "    w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "    i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "    w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "    i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "    save_pickle(vocab_w, 'pickle/vocab_w.pickle')\n",
    "    save_pickle(vocab_c, 'pickle/vocab_c.pickle')\n",
    "    save_pickle(w2i_w, 'pickle/w2i_w.pickle')\n",
    "    save_pickle(w2i_c, 'pickle/w2i_c.pickle')\n",
    "    save_pickle(i2w_w, 'pickle/i2w_w.pickle')\n",
    "    save_pickle(i2w_c, 'pickle/i2w_c.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _, _, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "# print('n_dev', len(dev_data))\n",
    "print('ctx_maxlen', ctx_maxlen)\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)\n",
    "\n",
    "if args.use_pickle == 1:\n",
    "    glove_embd_w = load_pickle('./pickle/glove_embd_w.pickle')\n",
    "else:\n",
    "    glove_embd_w = torch.from_numpy(load_glove_weights('./dataset', args.w_embd_size, vocab_size_w, w2i_w)).type(torch.FloatTensor)\n",
    "    save_pickle(glove_embd_w, './pickle/glove_embd_w.pickle')\n",
    "    \n",
    "# args = {\n",
    "#     'embd_size': embd_size,\n",
    "#     'vocab_size_c': vocab_size_c,\n",
    "#     'vocab_size_w': vocab_size_w,\n",
    "#     'pre_embd_w': glove_embd_w, # word embedding\n",
    "#     'filters': [[1, 5]], # char embedding\n",
    "#     'out_chs': 100, # char embedding\n",
    "#     'ans_size': ctx_maxlen\n",
    "# }\n",
    "# args = Config(**args)\n",
    "args.vocab_size_c = vocab_size_c\n",
    "args.vocab_size_w = vocab_size_w\n",
    "args.pre_embd_w = glove_embd_w\n",
    "args.filters = [[1, 5]]\n",
    "args.out_chs = 100\n",
    "args.ans_size = ctx_maxlen\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_ranking(p1, p2):\n",
    "    batch_size = p1.size(0)\n",
    "    p1_rank, p2_rank = [], []\n",
    "    for i in range(batch_size):\n",
    "        p1_rank.append(sorted(range(len(p1[i])), key=lambda k: p1[i][k].data[0], reverse=True))\n",
    "        p2_rank.append(sorted(range(len(p2[i])), key=lambda k: p2[i][k].data[0], reverse=True))\n",
    "    return p1_rank, p2_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Epoch 0\n",
      "[20171114-173935] 0.0%, loss_p1: 289.126, loss_p2: 481.591\n",
      "Rank 1, p1 result False, p2 result False\n",
      "[20171114-173949] 0.1%, loss_p1: 39.373, loss_p2: 28.451\n",
      "Rank 1, p1 result False, p2 result False\n",
      "[20171114-174002] 0.2%, loss_p1: 11.413, loss_p2: 9.413\n",
      "Rank 1, p1 result False, p2 result False\n",
      "[20171114-174015] 0.3%, loss_p1: 8.692, loss_p2: 10.749\n",
      "Rank 1, p1 result False, p2 result False\n",
      "[20171114-174029] 0.4%, loss_p1: 8.334, loss_p2: 8.323\n",
      "Rank 1, p1 result False, p2 result False\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, n_epoch=10, batch_size=1):\n",
    "    for epoch in range(n_epoch):\n",
    "        print('---Epoch', epoch)\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # TODO shuffle, last elms\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            c = [d[0] for d in batch_data]\n",
    "            cc = [d[1] for d in batch_data]\n",
    "            q = [d[3] for d in batch_data]\n",
    "            qc = [d[4] for d in batch_data]\n",
    "            a_beg = to_var(torch.LongTensor([d[6][0] for d in batch_data]).squeeze()) # TODO: multi target\n",
    "            a_end = to_var(torch.LongTensor([d[7][0] for d in batch_data]).squeeze()) \n",
    "            c_char_var = make_char_vector(cc, w2i_c, ctx_sent_maxlen, ctx_word_maxlen)\n",
    "            c_word_var = make_word_vector(c, w2i_w, ctx_sent_maxlen)\n",
    "            q_char_var = make_char_vector(qc, w2i_c, query_sent_maxlen, query_word_maxlen)\n",
    "            q_word_var = make_word_vector(q, w2i_w, query_sent_maxlen)\n",
    "            p1, p2 = model(c_char_var, c_word_var, q_char_var, q_word_var)\n",
    "            loss_p1 = nn.NLLLoss()(p1, a_beg)\n",
    "            loss_p2 = nn.NLLLoss()(p2, a_end)\n",
    "            if i % 100 == 0:\n",
    "                now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "                print('[{}] {:.1f}%, loss_p1: {:.3f}, loss_p2: {:.3f}'.format(now, 100*i/len(data), loss_p1.data[0], loss_p2.data[0]))\n",
    "                p1_rank, p2_rank = batch_ranking(p1, p2)\n",
    "                for rank in range(1): # N-best\n",
    "                    p1_rank_id = p1_rank[0][rank]\n",
    "                    p2_rank_id = p2_rank[0][rank]\n",
    "                    print('Rank {}, p1_result={}, p2_result={}'.format(\n",
    "                        rank+1, p1_rank_id==a_beg.data[0], p2_rank_id==a_end.data[0]))\n",
    "                \n",
    "            model.zero_grad()\n",
    "            (loss_p1+loss_p2).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "model = AttentionNet(args)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# print(model)\n",
    "optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=0.5, weight_decay=0.999)\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
