{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from process_data import save_pickle, load_pickle, load_task, load_glove_weights\n",
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: 1.1\n",
      "load_task: 0 / 442\n",
      "----\n",
      "n_train 269\n",
      "ctx_maxlen 1786\n",
      "vocab_size_w: 2783\n",
      "vocab_size_c: 89\n",
      "ctx_sent_maxlen: 333\n",
      "query_sent_maxlen: 25\n",
      "ctx_word_maxlen: 22\n",
      "query_word_maxlen: 14\n"
     ]
    }
   ],
   "source": [
    "train_data, train_ctx_maxlen = load_task('./dataset/train-v1.1.json')\n",
    "# dev_data = load_task('./dataset/dev-v1.1.json')\n",
    "data = train_data # + dev_data\n",
    "ctx_maxlen = train_ctx_maxlen\n",
    "# save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "# save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "vocab_w, vocab_c = set(), set()\n",
    "for ctx_w, ctx_c, q_id, q_w, q_c, answer, _, _ in data:\n",
    "    vocab_w |= set(ctx_w + q_w + answer)\n",
    "    flatten_c = [c for chars in ctx_c for c in chars]\n",
    "    flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "    vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "\n",
    "vocab_w = list(sorted(vocab_w))\n",
    "vocab_c = list(sorted(vocab_c))\n",
    "\n",
    "w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "# save_pickle(vocab, 'pickle/vocab.pickle')\n",
    "# save_pickle(w2i, 'pickle/w2i.pickle')\n",
    "# save_pickle(i2w, 'pickle/i2w.pickle')\n",
    "# train_data = load_pickle('pickle/train_data.pickle')\n",
    "# vocab = load_pickle('pickle/vocab.pickle')\n",
    "# w2i = load_pickle('pickle/w2i.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _, _, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "# print('n_dev', len(dev_data))\n",
    "print('ctx_maxlen', ctx_maxlen)\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (2783, 100)\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "glove_embd_w = torch.from_numpy(load_glove_weights('./dataset', embd_size, vocab_size_w, w2i_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx_embd_out torch.Size([16, 333, 400])\n",
      "ctx_embd_h torch.Size([2, 333, 200])\n",
      "ctx_embd_out torch.Size([16, 25, 400])\n",
      "ctx_embd_h torch.Size([2, 25, 200])\n",
      "-----------\n",
      "ctx_embd_context torch.Size([16, 333, 400])\n",
      "ctx_embd_query torch.Size([16, 25, 400])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4d7aaa14ccee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finish train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-4d7aaa14ccee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, n_epoch, batch_size)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mloss_p1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;31m#             loss_p2.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation"
     ]
    }
   ],
   "source": [
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "\n",
    "args = {\n",
    "    'embd_size': embd_size,\n",
    "    'vocab_size_c': vocab_size_c,\n",
    "    'vocab_size_w': vocab_size_w,\n",
    "    'pre_embd_w': glove_embd_w, # word embedding\n",
    "    'filters': [[1, 5]], # char embedding\n",
    "    'out_chs': 100, # char embedding\n",
    "    'ans_size': ctx_maxlen\n",
    "}\n",
    "args = Config(**args)\n",
    "\n",
    "def to_var(x):\n",
    "    # TODO CUDA\n",
    "    return Variable(x)\n",
    "\n",
    "def make_word_vector(data, w2i_w, query_len):\n",
    "    vec_data = []\n",
    "    for sentence in data:\n",
    "        index_vec = [w2i_w[w] for w in sentence]\n",
    "        pad_len = max(0, query_len - len(index_vec))\n",
    "        index_vec += [0] * pad_len\n",
    "        index_vec = index_vec[:query_len]\n",
    "        vec_data.append(index_vec)\n",
    "    \n",
    "    return to_var(torch.LongTensor(vec_data))\n",
    "\n",
    "def make_char_vector(data, w2i_c, query_len, word_len):\n",
    "    tmp = torch.zeros(len(data), query_len, word_len).type(torch.LongTensor)\n",
    "    for i, words in enumerate(data):\n",
    "        for j, word in enumerate(words):\n",
    "            for k, ch in enumerate(word):\n",
    "                tmp[i][j][k] = w2i_c[ch]\n",
    "    return to_var(tmp)\n",
    "\n",
    "def make_one_hot(data, data_size):\n",
    "    tmp = torch.zeros(len(data), data_size).type(torch.LongTensor)\n",
    "    for i in range(len(data)):\n",
    "        tmp[i, data[i][0]] = 1\n",
    "    return to_var(tmp)\n",
    "    \n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        self.embd_size = args.embd_size\n",
    "        self.char_embd_net = CharEmbedding(args)\n",
    "        self.word_embd_net = WordEmbedding(args)\n",
    "        self.highway_net = Highway(args.embd_size*2)# TODO share is ok?\n",
    "        self.ctx_embd_layer = nn.GRU(args.embd_size*2, args.embd_size*2, bidirectional=True)\n",
    "        self.W = nn.Parameter(torch.rand(3*2*2* args.embd_size).type(torch.FloatTensor).view(1, -1), requires_grad=True)\n",
    "#         self.beta = nn.Parameter(torch.rand(8*2*2* args.embd_size).type(torch.FloatTensor).view(1, -1), requires_grad=True)\n",
    "        self.modeling_layer = nn.GRU(args.embd_size*2*8, args.embd_size*2, bidirectional=True)\n",
    "        self.p1_layer = nn.Linear(args.embd_size*2*10, args.ans_size)\n",
    "        self.p2_lstm_layer = nn.GRU(args.embd_size*2*2, args.embd_size*2*2, bidirectional=True)\n",
    "        self.p2_layer = nn.Linear(args.embd_size*2*12, args.ans_size)\n",
    "    \n",
    "    def build_contextual_embd(self, x_c, x_w):\n",
    "        char_embd = self.char_embd_net(x_c) # (N, seq_len, embd_size)\n",
    "        word_embd = self.word_embd_net(x_w) # (N, seq_len, embd_size)\n",
    "        embd = torch.cat((char_embd, word_embd), 2) # (N, seq_len, embd_size*2)\n",
    "        embd = self.highway_net(embd)\n",
    "        ctx_embd_out, ctx_embd_h = self.ctx_embd_layer(embd)\n",
    "        print('ctx_embd_out', ctx_embd_out.size())\n",
    "        print('ctx_embd_h', ctx_embd_h.size())\n",
    "        return ctx_embd_out\n",
    "        \n",
    "    def forward(self, ctx_c, ctx_w, query_c, query_w):\n",
    "        batch_size = ctx_c.size(0)\n",
    "        embd_context = self.build_contextual_embd(ctx_c, ctx_w) # (N, T, 2d)\n",
    "        ctx_len = embd_context.size(1)\n",
    "        embd_query   = self.build_contextual_embd(query_c, query_w) # (N, J, 2d)\n",
    "        query_len = embd_query.size(1)\n",
    "        \n",
    "        # 4. Attention Flow Layer\n",
    "        # Context2Query\n",
    "        print('-----------')\n",
    "        print('ctx_embd_context', embd_context.size())\n",
    "        print('ctx_embd_query', embd_query.size())\n",
    "        a_elmwise_mul_b = Variable(torch.zeros(batch_size, ctx_len, query_len, 2*2*self.embd_size).type(torch.FloatTensor))\n",
    "        S = Variable(torch.zeros(batch_size, ctx_len, query_len).type(torch.FloatTensor))\n",
    "        for sample in range(batch_size): # TODO\n",
    "            for ci in range(ctx_len):\n",
    "                for qi in range(query_len):\n",
    "                    a_elmwise_mul_b[sample, ci, qi] = torch.mul(embd_context[sample, ci], embd_query[sample, qi])\n",
    "                    x = torch.cat((embd_context[sample, ci], embd_query[sample, qi], a_elmwise_mul_b[sample, ci, qi]), 0) # (1, 3*2*embd_dim)\n",
    "                    S[sample, ci, qi] = torch.mm(self.W, x.unsqueeze(1))[0][0]\n",
    "                    break\n",
    "                S[sample, ci] = F.softmax(S[sample, ci]) # softmax(in, dim) is only available in newer version                \n",
    "                break\n",
    "            break\n",
    "            \n",
    "        c2q = torch.bmm(S, embd_query) # (N, T, 2d)\n",
    "    \n",
    "        tmp_b = torch.max(S, 2)[0]\n",
    "        b = torch.stack([F.softmax(tmp_b[i]) for i in range(batch_size)], 0) # (N, T)\n",
    "        q2c = torch.bmm(b.unsqueeze(1), embd_context).squeeze() # (N, 2d)\n",
    "        q2c = q2c.unsqueeze(1) # (N, 1, 2d)\n",
    "        q2c = q2c.repeat(1, ctx_len, 1) # (N, T, 2d)\n",
    "        \n",
    "        G = torch.cat((embd_context, c2q, embd_context.mul(c2q), embd_context.mul(q2c)), 2) # (N, T, 8d)\n",
    "        \n",
    "        # 5. Modeling Layer\n",
    "        M, _ = self.modeling_layer(G) # M: (N, T, 2d)\n",
    "        \n",
    "        # 5. Output Layer\n",
    "        G_M = torch.cat((G, M), 2) # (N, T, 10d)\n",
    "        G_M = G_M.sum(1) #(N, 10d)\n",
    "        p1 = F.softmax(self.p1_layer(G_M)) # (N, T)\n",
    "        \n",
    "        M2, _ = self.p2_lstm_layer(M) # (N, T, 4d)\n",
    "        G_M2 = torch.cat((G, M2), 2) # (N, T, 12d)\n",
    "        G_M2 = G_M2.sum(1) # (N, 12d)(N, T)\n",
    "        p2 = F.softmax(self.p2_layer(G_M2)) # (N, T)\n",
    "        \n",
    "        return p1, p2\n",
    "        \n",
    "def train(model, optimizer, loss_fn, n_epoch=1, batch_size=16):\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # TODO shuffle, last elms\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            c = [d[0] for d in batch_data]\n",
    "            cc = [d[1] for d in batch_data]\n",
    "            q = [d[3] for d in batch_data]\n",
    "            qc = [d[4] for d in batch_data]\n",
    "            a_beg = to_var(torch.LongTensor([d[6] for d in batch_data]).squeeze())\n",
    "            a_end = to_var(torch.LongTensor([d[7] for d in batch_data]).squeeze())\n",
    "            c_char_var = make_char_vector(cc, w2i_c, ctx_sent_maxlen, ctx_word_maxlen)\n",
    "            c_word_var = make_word_vector(c, w2i_w, ctx_sent_maxlen)\n",
    "            q_char_var = make_char_vector(qc, w2i_c, query_sent_maxlen, query_word_maxlen)\n",
    "            q_word_var = make_word_vector(q, w2i_w, query_sent_maxlen)\n",
    "            p1, p2 = model(c_char_var, c_word_var, q_char_var, q_word_var)\n",
    "            \n",
    "            loss_p1 = loss_fn(p1, a_beg)\n",
    "#             loss_p2 = loss_fn(p2, a_end)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss_p1.backward()\n",
    "#             loss_p2.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            break\n",
    "\n",
    "model = AttentionNet(args)\n",
    "# print(model)\n",
    "optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=0.5)\n",
    "loss_fn = nn.NLLLoss()\n",
    "train(model, optimizer, loss_fn)\n",
    "print('finish train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # %load layers/char_embedding.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# # In : (N, sentence_len, word_len, vocab_size_c)\n",
    "# # Out: (N, sentence_len, embd_size)\n",
    "# class CharEmbedding(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(CharEmbedding, self).__init__()\n",
    "#         self.embd_size = args.embd_size\n",
    "#         self.embedding = nn.Embedding(args.vocab_size_c, args.embd_size)\n",
    "#         # nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
    "#         self.conv = nn.ModuleList([nn.Conv2d(1, args.out_chs, (f[0], f[1])) for f in args.filters])\n",
    "#         self.dropout = nn.Dropout(.5)\n",
    "#         self.fc1 = nn.Linear(args.out_chs*len(args.filters), 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: (N, seq_len, word_len)\n",
    "#         input_shape = x.size()\n",
    "#         bs = x.size(0)\n",
    "#         seq_len = x.size(1)\n",
    "#         word_len = x.size(2)\n",
    "#         x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
    "#         x = self.embedding(x) # (N*seq_len, word_len, embd_size)\n",
    "#         x = x.view(*input_shape, -1) # (N, seq_len, word_len, embd_size)\n",
    "#         x = x.sum(2) # (N, seq_len, embd_size)\n",
    "        \n",
    "#         return x\n",
    "# net = CharEmbedding(args)\n",
    "# bs = 10\n",
    "# seq_len = 7\n",
    "# word_len = 5\n",
    "# input = Variable(torch.zeros(bs, seq_len, word_len)).long()\n",
    "# out = net(input)\n",
    "# print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CharEmbedding test\n",
    "embd_size = 100\n",
    "n_out_ch = 100\n",
    "filters = [[1, 5]]\n",
    "tmp_data = data[0][4]\n",
    "max_len = max([len(chars) for chars in tmp_data])\n",
    "tmp_var = torch.zeros(1, query_sent_maxlen, query_word_maxlen).type(torch.LongTensor)\n",
    "print('tmp_var.size()=', tmp_var.size())\n",
    "for i, chars in enumerate(tmp_data):\n",
    "    for j, ch in enumerate(chars):\n",
    "        tmp_var[0][i][j] = w2i_c[ch]\n",
    "char_embd_net = CharEmbedding(vocab_size_c, embd_size, n_out_ch, filters)\n",
    "print(char_embd_net)\n",
    "out = char_embd_net(Variable(tmp_var))\n",
    "print(out)\n",
    "print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WordEmbedding Test\n",
    "word_embd_net = WordEmbedding(vocab_size_w, embd_size, False, glove_embd_w)\n",
    "word_var = Variable(torch.LongTensor([[w2i_w[w] for w in data[0][3]]]))\n",
    "out = word_embd_net(word_var)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embd_dim = 10\n",
    "a_len = 7\n",
    "b_len = 4\n",
    "\n",
    "a = torch.rand(batch_size, a_len, embd_dim).type(torch.DoubleTensor)  # dummy input1\n",
    "b = torch.rand(batch_size, b_len, embd_dim).type(torch.DoubleTensor)  # dummy input2\n",
    "# a_elmwise_mul_b: (N, a_len, b_len, embd_dim)   dummy-code\n",
    "a_elmwise_mul_b = torch.zeros(batch_size, a_len, b_len, embd_dim).type(torch.DoubleTensor)\n",
    "S = torch.zeros(batch_size, a_len, b_len).type(torch.DoubleTensor)\n",
    "W = torch.rand(3 * embd_dim).type(torch.DoubleTensor).view(1, -1) # must be trainable params\n",
    "# I think there are better way than below\n",
    "for sample in range(batch_size):\n",
    "    for ai in range(a_len):\n",
    "        for bi in range(b_len):\n",
    "            a_elmwise_mul_b[sample, ai, bi] = torch.mul(a[sample, ai], b[sample, bi])\n",
    "            x = torch.cat((a[sample, ai], b[sample, bi], a_elmwise_mul_b[sample, ai, bi])) # (1, 3*embd_dim)\n",
    "            S[sample, ai, bi] = torch.mm(W, x.unsqueeze(1))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embd_dim = 10\n",
    "a_len = 7\n",
    "b_len = 4\n",
    "\n",
    "a = torch.rand(batch_size, a_len, embd_dim).type(torch.DoubleTensor)  # dummy input1\n",
    "b = torch.rand(batch_size, b_len, embd_dim).type(torch.DoubleTensor)  # dummy input2\n",
    "# a_elmwise_mul_b: (N, a_len, b_len, embd_dim)   dummy-code\n",
    "a_elmwise_mul_b = torch.zeros(batch_size, a_len, b_len, embd_dim).type(torch.DoubleTensor)\n",
    "S = torch.zeros(batch_size, a_len, b_len).type(torch.DoubleTensor)\n",
    "W = torch.rand(3 * embd_dim).type(torch.DoubleTensor).view(1, -1) # must be trainable params\n",
    "# for sample in range(batch_size):\n",
    "#     for ai in range(a_len):\n",
    "#         for bi in range(b_len):\n",
    "#             a_elmwise_mul_b[sample, ai, bi] = torch.mul(a[sample, ai], b[sample, bi])\n",
    "#             x = torch.cat((a[sample, ai], b[sample, bi], a_elmwise_mul_b[sample, ai, bi])) # (1, 3*embd_dim)\n",
    "#             S[sample, ai, bi] = torch.mm(W, x.unsqueeze(1))[0][0]\n",
    "S = torch.bmm(a, b.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, _, _, _, _, _, _, _ in data:\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size N] [--test-batch-size N]\n",
      "                             [--epochs N] [--lr LR] [--momentum M] [--no-cuda]\n",
      "                             [--seed S] [--log-interval N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-a818d404-51e0-4797-9902-76133a624891.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        print(data.size())\n",
    "        print(target.size())\n",
    "        break\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
