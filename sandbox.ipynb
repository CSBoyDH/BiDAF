{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from process_data import save_pickle, load_pickle, load_task, load_glove_weights\n",
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: 1.1\n",
      "load_task: 0 / 442\n",
      "----\n",
      "n_train 269\n",
      "vocab_size_w: 2783\n",
      "vocab_size_c: 89\n",
      "ctx_sent_maxlen: 333\n",
      "query_sent_maxlen: 25\n",
      "ctx_word_maxlen: 22\n",
      "query_word_maxlen: 14\n"
     ]
    }
   ],
   "source": [
    "train_data = load_task('./dataset/train-v1.1.json')\n",
    "# dev_data = load_task('./dataset/dev-v1.1.json')\n",
    "data = train_data # + dev_data\n",
    "# save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "# save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "vocab_w, vocab_c = set(), set()\n",
    "for ctx_w, ctx_c, q_id, q_w, q_c, answer in data:\n",
    "    vocab_w |= set(ctx_w + q_w + answer)\n",
    "    flatten_c = [c for chars in ctx_c for c in chars]\n",
    "    flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "    vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "\n",
    "vocab_w = list(sorted(vocab_w))\n",
    "vocab_c = list(sorted(vocab_c))\n",
    "\n",
    "w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "# save_pickle(vocab, 'pickle/vocab.pickle')\n",
    "# save_pickle(w2i, 'pickle/w2i.pickle')\n",
    "# save_pickle(i2w, 'pickle/i2w.pickle')\n",
    "# train_data = load_pickle('pickle/train_data.pickle')\n",
    "# vocab = load_pickle('pickle/vocab.pickle')\n",
    "# w2i = load_pickle('pickle/w2i.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "# print('n_dev', len(dev_data))\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (2783, 100)\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "glove_embd_w = torch.from_numpy(load_glove_weights('./dataset', embd_size, vocab_size_w, w2i_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from layers.char_embedding import CharEmbedding\n",
    "# from layers.word_embedding import WordEmbedding\n",
    "# from layers.highway import Highway\n",
    "\n",
    "# args = {\n",
    "#     'embd_size': embd_size,\n",
    "#     'vocab_size_c': vocab_size_c,\n",
    "#     'vocab_size_w': vocab_size_w,\n",
    "#     'pre_embd_w': glove_embd_w, # word embedding\n",
    "#     'filters': [[1, 5]], # char embedding\n",
    "#     'out_chs': 100, # char embedding\n",
    "# }\n",
    "# conf = Config(**args)\n",
    "# def make_word_vector(data, w2i_w, query_len):\n",
    "#     vec_data = []\n",
    "#     for sentence in data:\n",
    "#         index_vec = [w2i_w[w] for w in sentence]\n",
    "#         pad_len = max(0, query_len - len(index_vec))\n",
    "#         index_vec += [0] * pad_len\n",
    "#         index_vec = index_vec[:query_len]\n",
    "#         vec_data.append(index_vec)\n",
    "    \n",
    "#     var = Variable(torch.LongTensor(vec_data))\n",
    "#     return var\n",
    "\n",
    "# def make_char_vector(data, w2i_c, query_len, word_len):\n",
    "#     tmp = torch.zeros(len(data), query_len, word_len).type(torch.LongTensor)\n",
    "#     for i, words in enumerate(data):\n",
    "#         for j, word in enumerate(words):\n",
    "#             for k, ch in enumerate(word):\n",
    "#                 tmp[i][j][k] = w2i_c[ch]\n",
    "#     return Variable(tmp)\n",
    "    \n",
    "# class AttentionNet(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(AttentionNet, self).__init__()\n",
    "#         self.embd_size = args.embd_size\n",
    "#         self.char_embd_net = CharEmbedding(args)\n",
    "#         self.word_embd_net = WordEmbedding(args)\n",
    "#         self.highway_net = Highway(args.embd_size*2)# TODO share is ok?\n",
    "#         self.ctx_embd_layer = nn.GRU(args.embd_size*2, args.embd_size*4)\n",
    "#         self.W = nn.Parameter(torch.rand(3 * args.embd_size).type(torch.DoubleTensor).view(1, -1), requires_grad=True)\n",
    "# #         self.W = Variable(torch.randn(2*2*args.embd_size),     requires_grad=True)\n",
    "#         print(self.W.size())\n",
    "#         print(type(self.W))\n",
    "    \n",
    "#     def build_contextual_embd(self, x_c, x_w):\n",
    "#         char_embd = self.char_embd_net(x_c) # (N, seq_len, embd_size)\n",
    "#         word_embd = self.word_embd_net(x_w) # (N, seq_len, embd_size)\n",
    "#         embd = torch.cat((char_embd, word_embd), 2) # (N, seq_len, embd_size*2)\n",
    "#         embd = self.highway_net(embd)\n",
    "#         ctx_embd_out, ctx_embd_h = self.ctx_embd_layer(embd)\n",
    "#         print('ctx_embd_out', ctx_embd_out.size())\n",
    "#         print('ctx_embd_h', ctx_embd_h.size())\n",
    "#         print('type', type(ctx_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1200])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "ctx_embd_out torch.Size([16, 333, 400])\n",
      "ctx_embd_h torch.Size([1, 333, 400])\n",
      "type <class 'torch.autograd.variable.Variable'>\n",
      "ctx_embd_out torch.Size([16, 25, 400])\n",
      "ctx_embd_h torch.Size([1, 25, 400])\n",
      "type <class 'torch.autograd.variable.Variable'>\n",
      "-----------\n",
      "ctx_embd_context torch.Size([16, 333, 400])\n",
      "ctx_embd_query torch.Size([16, 25, 400])\n"
     ]
    }
   ],
   "source": [
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "\n",
    "args = {\n",
    "    'embd_size': embd_size,\n",
    "    'vocab_size_c': vocab_size_c,\n",
    "    'vocab_size_w': vocab_size_w,\n",
    "    'pre_embd_w': glove_embd_w, # word embedding\n",
    "    'filters': [[1, 5]], # char embedding\n",
    "    'out_chs': 100, # char embedding\n",
    "}\n",
    "conf = Config(**args)\n",
    "def make_word_vector(data, w2i_w, query_len):\n",
    "    vec_data = []\n",
    "    for sentence in data:\n",
    "        index_vec = [w2i_w[w] for w in sentence]\n",
    "        pad_len = max(0, query_len - len(index_vec))\n",
    "        index_vec += [0] * pad_len\n",
    "        index_vec = index_vec[:query_len]\n",
    "        vec_data.append(index_vec)\n",
    "    \n",
    "    var = Variable(torch.LongTensor(vec_data))\n",
    "    return var\n",
    "\n",
    "def make_char_vector(data, w2i_c, query_len, word_len):\n",
    "    tmp = torch.zeros(len(data), query_len, word_len).type(torch.LongTensor)\n",
    "    for i, words in enumerate(data):\n",
    "        for j, word in enumerate(words):\n",
    "            for k, ch in enumerate(word):\n",
    "                tmp[i][j][k] = w2i_c[ch]\n",
    "    return Variable(tmp)\n",
    "    \n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        self.embd_size = args.embd_size\n",
    "        self.char_embd_net = CharEmbedding(args)\n",
    "        self.word_embd_net = WordEmbedding(args)\n",
    "        self.highway_net = Highway(args.embd_size*2)# TODO share is ok?\n",
    "        self.ctx_embd_layer = nn.GRU(args.embd_size*2, args.embd_size*4)\n",
    "        self.W = nn.Parameter(torch.rand(3*2*2* args.embd_size).type(torch.FloatTensor).view(1, -1), requires_grad=True)\n",
    "        print(self.W.size())\n",
    "        print(type(self.W))\n",
    "    \n",
    "    def build_contextual_embd(self, x_c, x_w):\n",
    "        char_embd = self.char_embd_net(x_c) # (N, seq_len, embd_size)\n",
    "        word_embd = self.word_embd_net(x_w) # (N, seq_len, embd_size)\n",
    "        embd = torch.cat((char_embd, word_embd), 2) # (N, seq_len, embd_size*2)\n",
    "        embd = self.highway_net(embd)\n",
    "        ctx_embd_out, ctx_embd_h = self.ctx_embd_layer(embd)\n",
    "        print('ctx_embd_out', ctx_embd_out.size())\n",
    "        print('ctx_embd_h', ctx_embd_h.size())\n",
    "        print('type', type(ctx_embd_out))\n",
    "        return ctx_embd_out\n",
    "        \n",
    "    def forward(self, ctx_c, ctx_w, query_c, query_w):\n",
    "        batch_size = ctx_c.size(0)\n",
    "        embd_context = self.build_contextual_embd(ctx_c, ctx_w) # (N, T, 2d)\n",
    "        ctx_len = embd_context.size(1)\n",
    "        embd_query   = self.build_contextual_embd(query_c, query_w) # (N, J, 2d)\n",
    "        query_len = embd_query.size(1)\n",
    "        \n",
    "        # Context2Query\n",
    "        print('-----------')\n",
    "        print('ctx_embd_context', embd_context.size())\n",
    "        print('ctx_embd_query', embd_query.size())\n",
    "        a_elmwise_mul_b = Variable(torch.zeros(batch_size, ctx_len, query_len, 2*2*self.embd_size).type(torch.FloatTensor))\n",
    "        S = Variable(torch.zeros(batch_size, ctx_len, query_len).type(torch.DoubleTensor))\n",
    "        for sample in range(batch_size): # TODO\n",
    "            for ai in range(ctx_len):\n",
    "                for bi in range(query_len):\n",
    "                    a_elmwise_mul_b[sample, ai, bi] = torch.mul(embd_context[sample, ai], embd_query[sample, bi])\n",
    "                    x = torch.cat((embd_context[sample, ai], embd_query[sample, bi], a_elmwise_mul_b[sample, ai, bi]), 0) # (1, 3*2*embd_dim)\n",
    "                    S[sample, ai, bi] = torch.mm(self.W, x.unsqueeze(1))[0][0]\n",
    "#         print('cat_data', cat_data.size())\n",
    "                \n",
    "def train(model, n_epoch=1, batch_size=16):\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # TODO shuffle, last elms\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            c = [d[0] for d in batch_data]\n",
    "            cc = [d[1] for d in batch_data]\n",
    "            q = [d[3] for d in batch_data]\n",
    "            qc = [d[4] for d in batch_data]\n",
    "            c_char_var = make_char_vector(cc, w2i_c, ctx_sent_maxlen, ctx_word_maxlen)\n",
    "            c_word_var = make_word_vector(c, w2i_w, ctx_sent_maxlen)\n",
    "            q_char_var = make_char_vector(qc, w2i_c, query_sent_maxlen, query_word_maxlen)\n",
    "            q_word_var = make_word_vector(q, w2i_w, query_sent_maxlen)\n",
    "            model(c_char_var, c_word_var, q_char_var, q_word_var)\n",
    "            break\n",
    "            \n",
    "attn = AttentionNet(conf)\n",
    "# print(attn)\n",
    "train(attn)\n",
    "print('finish train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # %load layers/char_embedding.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# # In : (N, sentence_len, word_len, vocab_size_c)\n",
    "# # Out: (N, sentence_len, embd_size)\n",
    "# class CharEmbedding(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(CharEmbedding, self).__init__()\n",
    "#         self.embd_size = args.embd_size\n",
    "#         self.embedding = nn.Embedding(args.vocab_size_c, args.embd_size)\n",
    "#         # nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
    "#         self.conv = nn.ModuleList([nn.Conv2d(1, args.out_chs, (f[0], f[1])) for f in args.filters])\n",
    "#         self.dropout = nn.Dropout(.5)\n",
    "#         self.fc1 = nn.Linear(args.out_chs*len(args.filters), 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: (N, seq_len, word_len)\n",
    "#         input_shape = x.size()\n",
    "#         bs = x.size(0)\n",
    "#         seq_len = x.size(1)\n",
    "#         word_len = x.size(2)\n",
    "#         x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
    "#         x = self.embedding(x) # (N*seq_len, word_len, embd_size)\n",
    "#         x = x.view(*input_shape, -1) # (N, seq_len, word_len, embd_size)\n",
    "#         x = x.sum(2) # (N, seq_len, embd_size)\n",
    "        \n",
    "#         return x\n",
    "# net = CharEmbedding(args)\n",
    "# bs = 10\n",
    "# seq_len = 7\n",
    "# word_len = 5\n",
    "# input = Variable(torch.zeros(bs, seq_len, word_len)).long()\n",
    "# out = net(input)\n",
    "# print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CharEmbedding test\n",
    "embd_size = 100\n",
    "n_out_ch = 100\n",
    "filters = [[1, 5]]\n",
    "tmp_data = data[0][4]\n",
    "max_len = max([len(chars) for chars in tmp_data])\n",
    "tmp_var = torch.zeros(1, query_sent_maxlen, query_word_maxlen).type(torch.LongTensor)\n",
    "print('tmp_var.size()=', tmp_var.size())\n",
    "for i, chars in enumerate(tmp_data):\n",
    "    for j, ch in enumerate(chars):\n",
    "        tmp_var[0][i][j] = w2i_c[ch]\n",
    "char_embd_net = CharEmbedding(vocab_size_c, embd_size, n_out_ch, filters)\n",
    "print(char_embd_net)\n",
    "out = char_embd_net(Variable(tmp_var))\n",
    "print(out)\n",
    "print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WordEmbedding Test\n",
    "word_embd_net = WordEmbedding(vocab_size_w, embd_size, False, glove_embd_w)\n",
    "word_var = Variable(torch.LongTensor([[w2i_w[w] for w in data[0][3]]]))\n",
    "out = word_embd_net(word_var)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embd_dim = 10\n",
    "a_len = 7\n",
    "b_len = 4\n",
    "\n",
    "a = torch.rand(batch_size, a_len, embd_dim).type(torch.DoubleTensor)  # dummy input1\n",
    "b = torch.rand(batch_size, b_len, embd_dim).type(torch.DoubleTensor)  # dummy input2\n",
    "# a_elmwise_mul_b: (N, a_len, b_len, embd_dim)   dummy-code\n",
    "a_elmwise_mul_b = torch.zeros(batch_size, a_len, b_len, embd_dim).type(torch.DoubleTensor)\n",
    "S = torch.zeros(batch_size, a_len, b_len).type(torch.DoubleTensor)\n",
    "W = torch.rand(3 * embd_dim).type(torch.DoubleTensor).view(1, -1) # must be trainable params\n",
    "# I think there are better way than below\n",
    "for sample in range(batch_size):\n",
    "    for ai in range(a_len):\n",
    "        for bi in range(b_len):\n",
    "            a_elmwise_mul_b[sample, ai, bi] = torch.mul(a[sample, ai], b[sample, bi])\n",
    "            x = torch.cat((a[sample, ai], b[sample, bi], a_elmwise_mul_b[sample, ai, bi])) # (1, 3*embd_dim)\n",
    "            S[sample, ai, bi] = torch.mm(W, x.unsqueeze(1))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embd_dim = 10\n",
    "a_len = 7\n",
    "b_len = 4\n",
    "\n",
    "a = torch.rand(batch_size, a_len, embd_dim).type(torch.DoubleTensor)  # dummy input1\n",
    "b = torch.rand(batch_size, b_len, embd_dim).type(torch.DoubleTensor)  # dummy input2\n",
    "# a_elmwise_mul_b: (N, a_len, b_len, embd_dim)   dummy-code\n",
    "a_elmwise_mul_b = torch.zeros(batch_size, a_len, b_len, embd_dim).type(torch.DoubleTensor)\n",
    "S = torch.zeros(batch_size, a_len, b_len).type(torch.DoubleTensor)\n",
    "W = torch.rand(3 * embd_dim).type(torch.DoubleTensor).view(1, -1) # must be trainable params\n",
    "# for sample in range(batch_size):\n",
    "#     for ai in range(a_len):\n",
    "#         for bi in range(b_len):\n",
    "#             a_elmwise_mul_b[sample, ai, bi] = torch.mul(a[sample, ai], b[sample, bi])\n",
    "#             x = torch.cat((a[sample, ai], b[sample, bi], a_elmwise_mul_b[sample, ai, bi])) # (1, 3*embd_dim)\n",
    "#             S[sample, ai, bi] = torch.mm(W, x.unsqueeze(1))[0][0]\n",
    "S = torch.bmm(a, b.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
