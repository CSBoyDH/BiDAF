{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loads():\n",
    "    pass\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from process_data import save_pickle, load_pickle, load_task, load_glove_weights\n",
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "from config import Config\n",
    "loads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: 1.1\n",
      "load_task: 0 / 442\n",
      "----\n",
      "n_train 269\n",
      "vocab_size_w: 2783\n",
      "vocab_size_c: 89\n",
      "ctx_sent_maxlen: 333\n",
      "query_sent_maxlen: 25\n",
      "ctx_word_maxlen: 22\n",
      "query_word_maxlen: 14\n"
     ]
    }
   ],
   "source": [
    "train_data = load_task('./dataset/train-v1.1.json')\n",
    "# dev_data = load_task('./dataset/dev-v1.1.json')\n",
    "data = train_data # + dev_data\n",
    "# save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "# save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "vocab_w, vocab_c = set(), set()\n",
    "for ctx_w, ctx_c, q_id, q_w, q_c, answer in data:\n",
    "    vocab_w |= set(ctx_w + q_w + answer)\n",
    "    flatten_c = [c for chars in ctx_c for c in chars]\n",
    "    flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "    vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "\n",
    "vocab_w = list(sorted(vocab_w))\n",
    "vocab_c = list(sorted(vocab_c))\n",
    "\n",
    "w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "# save_pickle(vocab, 'pickle/vocab.pickle')\n",
    "# save_pickle(w2i, 'pickle/w2i.pickle')\n",
    "# save_pickle(i2w, 'pickle/i2w.pickle')\n",
    "# train_data = load_pickle('pickle/train_data.pickle')\n",
    "# vocab = load_pickle('pickle/vocab.pickle')\n",
    "# w2i = load_pickle('pickle/w2i.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "# print('n_dev', len(dev_data))\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (2783, 100)\n"
     ]
    }
   ],
   "source": [
    "glove_embd_w = torch.from_numpy(load_glove_weights('./dataset', embd_size, vocab_size_w, w2i_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_var torch.Size([16, 25])\n",
      "char_var torch.Size([16, 25, 14])\n",
      "embd torch.Size([16, 25, 200])\n"
     ]
    }
   ],
   "source": [
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "\n",
    "args = {\n",
    "    'embd_size': embd_size,\n",
    "    'vocab_size_c': vocab_size_c,\n",
    "    'vocab_size_w': vocab_size_w,\n",
    "    'pre_embd_w': glove_embd_w,\n",
    "    'filters': [[1, 5]], # char embedding\n",
    "    'out_chs': 100, # char embedding\n",
    "}\n",
    "conf = Config(**args)\n",
    "def make_word_vector(data, w2i_w, query_len):\n",
    "    vec_data = []\n",
    "    for sentence in data:\n",
    "        index_vec = [w2i_w[w] for w in sentence]\n",
    "        pad_len = max(0, query_len - len(index_vec))\n",
    "        index_vec += [0] * pad_len\n",
    "        index_vec = index_vec[:query_len]\n",
    "        vec_data.append(index_vec)\n",
    "    \n",
    "    var = Variable(torch.LongTensor(vec_data))\n",
    "    return var\n",
    "\n",
    "def make_char_vector(data, w2i_c, query_len, word_len):\n",
    "    tmp = torch.zeros(len(data), query_len, word_len).type(torch.LongTensor)\n",
    "    for i, words in enumerate(data):\n",
    "        for j, word in enumerate(words):\n",
    "            for k, ch in enumerate(word):\n",
    "                tmp[i][j][k] = w2i_c[ch]\n",
    "    return Variable(tmp)\n",
    "    \n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        \n",
    "        self.char_embd_net = CharEmbedding(args)\n",
    "        self.word_embd_net = WordEmbedding(args)\n",
    "    \n",
    "    def forward(self, x_c, x_w):\n",
    "        char_embd = self.char_embd_net(x_c) # (N, seq_len, embd_size)\n",
    "        print('char_embd', char_embd.size())\n",
    "        word_embd = self.word_embd_net(x_w) # (N, seq_len, embd_size)\n",
    "        print('word_embd', word_embd.size())\n",
    "        embd = torch.cat((char_embd, word_embd), 2) # (N, seq_len, embd_size*2)\n",
    "        print('embd', embd.size())\n",
    "        \n",
    "        \n",
    "def train(model, n_epoch=1, batch_size=16):\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # TODO shuffle, last elms\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            q = [d[3] for d in batch_data]\n",
    "            qc = [d[4] for d in batch_data]\n",
    "            word_var = make_word_vector(q, w2i_w, query_sent_maxlen)\n",
    "            char_var = make_char_vector(qc, w2i_c, query_sent_maxlen, query_word_maxlen)\n",
    "            model(char_var, word_var)\n",
    "            break\n",
    "            \n",
    "attn = AttentionNet(conf)\n",
    "train(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q ['To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes', 'France', '?']\n",
      "qc [['T', 'o'], ['w', 'h', 'o', 'm'], ['d', 'i', 'd'], ['t', 'h', 'e'], ['V', 'i', 'r', 'g', 'i', 'n'], ['M', 'a', 'r', 'y'], ['a', 'l', 'l', 'e', 'g', 'e', 'd', 'l', 'y'], ['a', 'p', 'p', 'e', 'a', 'r'], ['i', 'n'], ['1', '8', '5', '8'], ['i', 'n'], ['L', 'o', 'u', 'r', 'd', 'e', 's'], ['F', 'r', 'a', 'n', 'c', 'e'], ['?']]\n"
     ]
    }
   ],
   "source": [
    "print('q', data[0][3])\n",
    "print('qc', data[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_var.size()= torch.Size([1, 25, 14])\n",
      "CharEmbedding (\n",
      "  (embedding): Embedding(89, 100)\n",
      "  (conv): ModuleList (\n",
      "    (0): Conv2d(1, 100, kernel_size=(1, 5), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout (p = 0.5)\n",
      "  (fc1): Linear (100 -> 1)\n",
      ")\n",
      "x torch.Size([1, 25, 14])\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  247.3019  319.3441  262.0004  ...   330.8669  262.8564  279.7697\n",
      "  290.2071  286.9221  323.5995  ...   282.1275  306.3464  255.2149\n",
      "  208.0627  219.9793  210.1831  ...   224.5885  210.1546  189.7299\n",
      "              ...                â‹±                ...             \n",
      "  171.9374  220.1119  190.7723  ...   224.9468  207.3729  211.4682\n",
      "  229.2606  220.2159  259.0305  ...   202.1327  247.5456  205.9599\n",
      "  234.0376  280.7905  357.0085  ...   291.7557  364.4121  227.2117\n",
      "[torch.FloatTensor of size 1x25x100]\n",
      "\n",
      "out torch.Size([1, 25, 100])\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "n_out_ch = 100\n",
    "filters = [[1, 5]]\n",
    "tmp_data = data[0][4]\n",
    "max_len = max([len(chars) for chars in tmp_data])\n",
    "tmp_var = torch.zeros(1, query_sent_maxlen, query_word_maxlen).type(torch.LongTensor)\n",
    "print('tmp_var.size()=', tmp_var.size())\n",
    "for i, chars in enumerate(tmp_data):\n",
    "    for j, ch in enumerate(chars):\n",
    "        tmp_var[0][i][j] = w2i_c[ch]\n",
    "char_embd_net = CharEmbedding(vocab_size_c, embd_size, n_out_ch, filters)\n",
    "print(char_embd_net)\n",
    "out = char_embd_net(Variable(tmp_var))\n",
    "print(out)\n",
    "print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out torch.Size([1, 14, 100])\n",
      "torch.Size([1, 14, 100])\n"
     ]
    }
   ],
   "source": [
    "word_embd_net = WordEmbedding(vocab_size_w, embd_size, False, glove_embd_w)\n",
    "word_var = Variable(torch.LongTensor([[w2i_w[w] for w in data[0][3]]]))\n",
    "out = word_embd_net(word_var)\n",
    "print(out.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "a={'test':2}\n",
    "a = Config(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['aaaa', 'bbb', 'ccc'], ['a', 'b', 'c']),\n",
       " (['aaaa', 'bbb', 'ccc'], ['a', 'b', 'c'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\n",
    "    (['aaaa', 'bbb', 'ccc'], ['a','b','c']),\n",
    "    (['aaaa', 'bbb', 'ccc'], ['a','b','c'])\n",
    "]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.7367  1.6024  1.0078\n",
      " 0.0680  0.0471  2.3368\n",
      "[torch.FloatTensor of size 2x3]\n",
      " \n",
      " 0.4483 -1.5221 -0.9414\n",
      "-0.2357 -0.0590  0.1027\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.7367  1.6024  1.0078  0.4483 -1.5221 -0.9414\n",
       " 0.0680  0.0471  2.3368 -0.2357 -0.0590  0.1027\n",
       "[torch.FloatTensor of size 2x6]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
