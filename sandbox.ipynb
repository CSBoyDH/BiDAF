{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from process_data import save_pickle, load_pickle, load_task, load_glove_weights\n",
    "from process_data import to_var, make_word_vector, make_char_vector\n",
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: 1.1\n",
      "load_task: 0 / 442\n",
      "----\n",
      "n_train 269\n",
      "ctx_maxlen 1786\n",
      "vocab_size_w: 2783\n",
      "vocab_size_c: 89\n",
      "ctx_sent_maxlen: 333\n",
      "query_sent_maxlen: 25\n",
      "ctx_word_maxlen: 22\n",
      "query_word_maxlen: 14\n"
     ]
    }
   ],
   "source": [
    "train_data, train_ctx_maxlen = load_task('./dataset/train-v1.1.json')\n",
    "# dev_data = load_task('./dataset/dev-v1.1.json')\n",
    "data = train_data # + dev_data\n",
    "ctx_maxlen = train_ctx_maxlen\n",
    "# save_pickle(train_data, 'pickle/train_data.pickle')\n",
    "# save_pickle(dev_data, 'pickle/dev_data.pickle')\n",
    "\n",
    "vocab_w, vocab_c = set(), set()\n",
    "for ctx_w, ctx_c, q_id, q_w, q_c, answer, _, _ in data:\n",
    "    vocab_w |= set(ctx_w + q_w + answer)\n",
    "    flatten_c = [c for chars in ctx_c for c in chars]\n",
    "    flatten_q = [c for chars in q_c for c in chars]\n",
    "\n",
    "    vocab_c |= set(flatten_c + flatten_q) # TODO\n",
    "\n",
    "vocab_w = list(sorted(vocab_w))\n",
    "vocab_c = list(sorted(vocab_c))\n",
    "\n",
    "w2i_w = dict((w, i) for i, w in enumerate(vocab_w, 0))\n",
    "i2w_w = dict((i, w) for i, w in enumerate(vocab_w, 0))\n",
    "w2i_c = dict((c, i) for i, c in enumerate(vocab_c, 0))\n",
    "i2w_c = dict((i, c) for i, c in enumerate(vocab_c, 0))\n",
    "# save_pickle(vocab, 'pickle/vocab.pickle')\n",
    "# save_pickle(w2i, 'pickle/w2i.pickle')\n",
    "# save_pickle(i2w, 'pickle/i2w.pickle')\n",
    "# train_data = load_pickle('pickle/train_data.pickle')\n",
    "# vocab = load_pickle('pickle/vocab.pickle')\n",
    "# w2i = load_pickle('pickle/w2i.pickle')\n",
    "\n",
    "vocab_size_w = len(vocab_w)\n",
    "vocab_size_c = len(vocab_c)\n",
    "\n",
    "ctx_sent_maxlen = max([len(c) for c, _, _, _, _, _, _, _ in data])\n",
    "query_sent_maxlen = max([len(q) for _, _, _, q, _, _, _, _ in data])\n",
    "ctx_word_maxlen = max([len(w) for _, cc, _, _, _, _, _, _ in data for w in cc])\n",
    "query_word_maxlen = max([len(w) for _, _, _, _, qc, _, _, _ in data for w in qc])\n",
    "print('----')\n",
    "print('n_train', len(train_data))\n",
    "# print('n_dev', len(dev_data))\n",
    "print('ctx_maxlen', ctx_maxlen)\n",
    "print('vocab_size_w:', vocab_size_w)\n",
    "print('vocab_size_c:', vocab_size_c)\n",
    "print('ctx_sent_maxlen:', ctx_sent_maxlen)\n",
    "print('query_sent_maxlen:', query_sent_maxlen)\n",
    "print('ctx_word_maxlen:', ctx_word_maxlen)\n",
    "print('query_word_maxlen:', query_word_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (2783, 100)\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "glove_embd_w = torch.from_numpy(load_glove_weights('./dataset', embd_size, vocab_size_w, w2i_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_p1.backward()\n",
      "loss_p2.backward()\n"
     ]
    }
   ],
   "source": [
    "from layers.char_embedding import CharEmbedding\n",
    "from layers.word_embedding import WordEmbedding\n",
    "from layers.highway import Highway\n",
    "\n",
    "args = {\n",
    "    'embd_size': embd_size,\n",
    "    'vocab_size_c': vocab_size_c,\n",
    "    'vocab_size_w': vocab_size_w,\n",
    "    'pre_embd_w': glove_embd_w, # word embedding\n",
    "    'filters': [[1, 5]], # char embedding\n",
    "    'out_chs': 100, # char embedding\n",
    "    'ans_size': ctx_maxlen\n",
    "}\n",
    "args = Config(**args)\n",
    "\n",
    "\n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        self.embd_size = args.embd_size\n",
    "        self.ans_size = args.ans_size\n",
    "        self.char_embd_net = CharEmbedding(args)\n",
    "        self.word_embd_net = WordEmbedding(args)\n",
    "        self.highway_net = Highway(args.embd_size*2)# TODO share is ok?\n",
    "        self.ctx_embd_layer = nn.GRU(args.embd_size*2, args.embd_size*2, bidirectional=True)\n",
    "        self.W = nn.Parameter(torch.rand(3*2*2* args.embd_size).type(torch.FloatTensor).view(1, -1), requires_grad=True)\n",
    "#         self.beta = nn.Parameter(torch.rand(8*2*2* args.embd_size).type(torch.FloatTensor).view(1, -1), requires_grad=True)\n",
    "        self.modeling_layer = nn.GRU(args.embd_size*2*8, args.embd_size*2, bidirectional=True)\n",
    "        self.p1_layer = nn.Linear(args.embd_size*2*10, args.ans_size)\n",
    "        self.p2_lstm_layer = nn.GRU(args.embd_size*2*2, args.embd_size*2*2, bidirectional=True)\n",
    "        self.p2_layer = nn.Linear(args.embd_size*2*12, args.ans_size)\n",
    "        \n",
    "        self.tmp_layer = nn.Linear(333*400, args.ans_size)\n",
    "    \n",
    "    def build_contextual_embd(self, x_c, x_w):\n",
    "        # 1. Caracter Embedding Layer\n",
    "        char_embd = self.char_embd_net(x_c) # (N, seq_len, embd_size)\n",
    "        if torch.cuda.is_available():\n",
    "            char_embd = char_embd.cuda()\n",
    "        # 2. Word Embedding Layer\n",
    "        word_embd = self.word_embd_net(x_w) # (N, seq_len, embd_size)\n",
    "        if torch.cuda.is_available():\n",
    "            word_embd = word_embd.cuda()\n",
    "        # Highway Networks of 1. and 2.\n",
    "        embd = torch.cat((char_embd, word_embd), 2) # (N, seq_len, embd_size*2)\n",
    "        embd = self.highway_net(embd)\n",
    "        \n",
    "        # 3. Contextual  Embedding Layer\n",
    "        ctx_embd_out, ctx_embd_h = self.ctx_embd_layer(embd)\n",
    "        return ctx_embd_out\n",
    "        \n",
    "    def forward(self, ctx_c, ctx_w, query_c, query_w):\n",
    "        batch_size = ctx_c.size(0)\n",
    "        \n",
    "        # 1. Caracter Embedding Layer \n",
    "        # 2. Word Embedding Layer\n",
    "        # 3. Contextual  Embedding Layer\n",
    "        embd_context = self.build_contextual_embd(ctx_c, ctx_w) # (N, T, 2d)\n",
    "        ctx_len = embd_context.size(1)\n",
    "        embd_query   = self.build_contextual_embd(query_c, query_w) # (N, J, 2d)\n",
    "        query_len = embd_query.size(1)\n",
    "        \n",
    "        # 4. Attention Flow Layer\n",
    "        # Context2Query\n",
    "        a_elmwise_mul_b = to_var(torch.zeros(batch_size, ctx_len, query_len, 2*2*self.embd_size).type(torch.FloatTensor))\n",
    "        S = to_var(torch.zeros(batch_size, ctx_len, query_len).type(torch.FloatTensor))\n",
    "        for sample in range(batch_size): # TODO\n",
    "            for ci in range(ctx_len):\n",
    "                for qi in range(query_len):\n",
    "                    a_elmwise_mul_b[sample, ci, qi] = torch.mul(embd_context[sample, ci], embd_query[sample, qi]).clone()\n",
    "                    x = torch.cat((embd_context[sample, ci], embd_query[sample, qi], a_elmwise_mul_b[sample, ci, qi]), 0) # (1, 3*2*embd_dim)\n",
    "                    S[sample, ci, qi] = torch.mm(self.W, x.unsqueeze(1))[0][0].clone()\n",
    "                S[sample, ci] = F.softmax(S[sample, ci].clone()) # softmax(in, dim) is only available in newer version                \n",
    "            \n",
    "        c2q = torch.bmm(S, embd_query) # (N, T, 2d)\n",
    "        \n",
    "        # Query2Context\n",
    "        tmp_b = torch.max(S, 2)[0]\n",
    "        b = torch.stack([F.softmax(tmp_b[i]) for i in range(batch_size)], 0) # (N, T)\n",
    "        q2c = torch.bmm(b.unsqueeze(1), embd_context).squeeze() # (N, 2d)\n",
    "        q2c = q2c.unsqueeze(1) # (N, 1, 2d)\n",
    "        q2c = q2c.repeat(1, ctx_len, 1) # (N, T, 2d)\n",
    "        \n",
    "        G = torch.cat((embd_context, c2q, embd_context.mul(c2q), embd_context.mul(q2c)), 2) # (N, T, 8d)\n",
    "        \n",
    "        # 5. Modeling Layer\n",
    "        M, _ = self.modeling_layer(G) # M: (N, T, 2d)\n",
    "        \n",
    "        # 5. Output Layer\n",
    "        G_M = torch.cat((G, M), 2) # (N, T, 10d)\n",
    "        G_M = G_M.sum(1) #(N, 10d)\n",
    "        p1 = F.softmax(self.p1_layer(G_M)) # (N, T)\n",
    "        \n",
    "        M2, _ = self.p2_lstm_layer(M) # (N, T, 4d)\n",
    "        G_M2 = torch.cat((G, M2), 2) # (N, T, 12d)\n",
    "        G_M2 = G_M2.sum(1) # (N, 12d)(N, T)\n",
    "        p2 = F.softmax(self.p2_layer(G_M2)) # (N, T)\n",
    "        \n",
    "        return p1, p2\n",
    "        \n",
    "def train(model, optimizer, n_epoch=1, batch_size=16):\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # TODO shuffle, last elms\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            c = [d[0] for d in batch_data]\n",
    "            cc = [d[1] for d in batch_data]\n",
    "            q = [d[3] for d in batch_data]\n",
    "            qc = [d[4] for d in batch_data]\n",
    "            a_beg = to_var(torch.LongTensor([d[6] for d in batch_data]).squeeze())\n",
    "            a_end = to_var(torch.LongTensor([d[7] for d in batch_data]).squeeze())\n",
    "            c_char_var = make_char_vector(cc, w2i_c, ctx_sent_maxlen, ctx_word_maxlen)\n",
    "            c_word_var = make_word_vector(c, w2i_w, ctx_sent_maxlen)\n",
    "            q_char_var = make_char_vector(qc, w2i_c, query_sent_maxlen, query_word_maxlen)\n",
    "            q_word_var = make_word_vector(q, w2i_w, query_sent_maxlen)\n",
    "            p1, p2 = model(c_char_var, c_word_var, q_char_var, q_word_var)\n",
    "            loss_p1 = nn.NLLLoss()(p1, a_beg)\n",
    "            loss_p2 = nn.NLLLoss()(p2, a_end)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            print('loss.backward()')\n",
    "            (loss_p1+loss_p2).backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            break\n",
    "\n",
    "model = AttentionNet(args)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "print(model)\n",
    "optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=0.5)\n",
    "train(model, optimizer)\n",
    "print('finish train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
