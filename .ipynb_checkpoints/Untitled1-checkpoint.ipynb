{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters----------\n",
      "Parameter containing:\n",
      " 0.1490  1.8259 -1.9674 -0.3149 -1.3276\n",
      "-1.6764 -1.1472  1.8022 -0.3865  0.2757\n",
      " 0.8690 -0.0398  1.9886  0.3197 -0.4861\n",
      " 0.5454  2.1086 -0.3124  1.8745 -0.1813\n",
      "-0.4172  1.4982  1.4596 -0.9093  0.2833\n",
      "-0.3211  0.9350 -1.0246  0.9142  0.4546\n",
      " 1.4384  0.8930  0.4210  1.0560 -0.3990\n",
      "-1.0192  0.2614 -0.0097  0.6511 -1.2873\n",
      "-2.7585  0.2278 -0.1578 -0.4071  0.3410\n",
      "-0.2571  0.0150 -0.6115 -1.6582  0.6026\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n",
      "loss: 26.306886672973633\n",
      "loss: 2.7677154541015625\n",
      "loss: 1.6735713481903076\n",
      "loss: 1.6094379425048828\n",
      "loss: 1.6094379425048828\n",
      "loss: 1.6094379425048828\n",
      "loss: 1.6094379425048828\n",
      "loss: 1.6094379425048828\n",
      "loss: 1.6094379425048828\n",
      "loss: 1.6094379425048828\n",
      "======Test======\n",
      "False out 0 label 3\n",
      "False out 0 label 2\n",
      "False out 0 label 3\n",
      "False out 0 label 4\n",
      "True out 3 label 3\n",
      "False out 0 label 4\n",
      "False out 0 label 3\n",
      "False out 0 label 2\n",
      "False out 0 label 4\n",
      "False out 0 label 3\n",
      "after parameters\n",
      "Parameter containing:\n",
      "-0.2575  0.1158 -2.0486 -0.4381 -1.3555\n",
      "-1.7109 -1.6755  0.6913 -0.8500  0.0524\n",
      " 0.5923 -0.6793  0.9087 -0.2320 -0.5682\n",
      " 0.3544  0.9175 -0.7964  1.0369 -0.3128\n",
      "-0.5666  0.2319  0.5246 -0.9790  0.0659\n",
      "-0.4442 -0.0965 -1.3729  0.2017  0.1900\n",
      " 1.0994  0.0795 -0.1975  0.3239 -0.5097\n",
      "-1.0573 -0.5273 -0.6530 -0.0484 -1.3178\n",
      "-2.7584 -0.6932 -0.7761 -0.8731  0.0945\n",
      "-0.4330 -0.9004 -1.1826 -1.8265  0.3127\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "n_class = 5\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(seq_len, n_class), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(x, self.W)\n",
    "        x = F.relu(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print('parameters----------')\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.0001)\n",
    "loss_fn = nn.NLLLoss()\n",
    "for i in range(10000):\n",
    "    in_data = Variable(torch.FloatTensor([[random.randint(0, n_class-1) for _ in range(seq_len)]]))\n",
    "    target_data = Variable(torch.LongTensor([ int(in_data.data[0][0]) ]))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(in_data)\n",
    "    loss = loss_fn(output, target_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print('loss:', loss.data[0])\n",
    "    \n",
    "# test\n",
    "print('======Test======')\n",
    "for i in range(10):\n",
    "    in_data = Variable(torch.FloatTensor([[random.randint(0, n_class-1) for _ in range(seq_len)]]))\n",
    "    target_data = int(in_data.data[0][0])\n",
    "    out = model(in_data)\n",
    "    _, ind = torch.max(out.data[0], 0)\n",
    "    print(ind[0] == target_data, 'out', ind[0], 'label', target_data)\n",
    "\n",
    "print('after parameters')\n",
    "print(next(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(0, 4) &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(random.randint(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.7355\n",
      "-3.6870\n",
      "-4.6384\n",
      "-5.6850\n",
      "-5.8753\n",
      "-3.4863\n",
      "-8.3395\n",
      "-5.2283\n",
      "-6.4461\n",
      "-1.7555\n",
      "-5.9721\n",
      "-9.2148\n",
      "-4.4767\n",
      "-6.7982\n",
      "-2.5625\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-2.3812\n",
      "-3.2195\n",
      "-4.0578\n",
      "-4.9799\n",
      "-5.1476\n",
      "-3.0427\n",
      "-7.3187\n",
      "-4.5775\n",
      "-5.6505\n",
      "-1.5178\n",
      "-5.2329\n",
      "-8.0900\n",
      "-3.9153\n",
      "-5.9607\n",
      "-2.2288\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-2.0575\n",
      "-2.7924\n",
      "-3.5273\n",
      "-4.3357\n",
      "-4.4827\n",
      "-2.6374\n",
      "-6.3862\n",
      "-3.9830\n",
      "-4.9237\n",
      "-1.3005\n",
      "-4.5576\n",
      "-7.0623\n",
      "-3.4024\n",
      "-5.1956\n",
      "-1.9238\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.7617\n",
      "-2.4022\n",
      "-3.0427\n",
      "-3.7472\n",
      "-3.8753\n",
      "-2.2671\n",
      "-5.5342\n",
      "-3.4398\n",
      "-4.2596\n",
      "-1.1020\n",
      "-3.9406\n",
      "-6.1235\n",
      "-2.9338\n",
      "-4.4966\n",
      "-1.6453\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.4915\n",
      "-2.0457\n",
      "-2.5999\n",
      "-3.2096\n",
      "-3.3204\n",
      "-1.9288\n",
      "-4.7559\n",
      "-2.9436\n",
      "-3.6530\n",
      "-0.9207\n",
      "-3.3769\n",
      "-5.2657\n",
      "-2.5057\n",
      "-3.8580\n",
      "-1.3907\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [5/60], Loss: 30.5370\n",
      "Variable containing:\n",
      "-1.2446\n",
      "-1.7200\n",
      "-2.1954\n",
      "-2.7184\n",
      "-2.8135\n",
      "-1.6198\n",
      "-4.0448\n",
      "-2.4902\n",
      "-3.0987\n",
      "-0.7550\n",
      "-2.8619\n",
      "-4.4821\n",
      "-2.1146\n",
      "-3.2746\n",
      "-1.1582\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.0191\n",
      "-1.4225\n",
      "-1.8259\n",
      "-2.2696\n",
      "-2.3503\n",
      "-1.3374\n",
      "-3.3951\n",
      "-2.0760\n",
      "-2.5923\n",
      "-0.6036\n",
      "-2.3914\n",
      "-3.7662\n",
      "-1.7573\n",
      "-2.7416\n",
      "-0.9457\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.8130\n",
      "-1.1506\n",
      "-1.4882\n",
      "-1.8596\n",
      "-1.9271\n",
      "-1.0794\n",
      "-2.8016\n",
      "-1.6976\n",
      "-2.1297\n",
      "-0.4653\n",
      "-1.9615\n",
      "-3.1122\n",
      "-1.4308\n",
      "-2.2546\n",
      "-0.7516\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.6247\n",
      "-0.9023\n",
      "-1.1798\n",
      "-1.4850\n",
      "-1.5405\n",
      "-0.8437\n",
      "-2.2593\n",
      "-1.3518\n",
      "-1.7070\n",
      "-0.3389\n",
      "-1.5688\n",
      "-2.5146\n",
      "-1.1326\n",
      "-1.8097\n",
      "-0.5743\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.4527\n",
      "-0.6753\n",
      "-0.8980\n",
      "-1.1428\n",
      "-1.1873\n",
      "-0.6284\n",
      "-1.7639\n",
      "-1.0360\n",
      "-1.3209\n",
      "-0.2235\n",
      "-1.2100\n",
      "-1.9687\n",
      "-0.8601\n",
      "-1.4033\n",
      "-0.4123\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [10/60], Loss: 12.4877\n",
      "Variable containing:\n",
      "-0.2956\n",
      "-0.4680\n",
      "-0.6405\n",
      "-0.8302\n",
      "-0.8647\n",
      "-0.4317\n",
      "-1.3113\n",
      "-0.7474\n",
      "-0.9681\n",
      "-0.1180\n",
      "-0.8822\n",
      "-1.4700\n",
      "-0.6112\n",
      "-1.0319\n",
      "-0.2642\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.1520\n",
      "-0.2786\n",
      "-0.4053\n",
      "-0.5445\n",
      "-0.5699\n",
      "-0.2519\n",
      "-0.8978\n",
      "-0.4838\n",
      "-0.6458\n",
      "-0.0216\n",
      "-0.5828\n",
      "-1.0143\n",
      "-0.3837\n",
      "-0.6927\n",
      "-0.1290\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.0208\n",
      "-0.1056\n",
      "-0.1903\n",
      "-0.2836\n",
      "-0.3005\n",
      "-0.0877\n",
      "-0.5201\n",
      "-0.2429\n",
      "-0.3514\n",
      " 0.0665\n",
      "-0.3092\n",
      "-0.5980\n",
      "-0.1759\n",
      "-0.3827\n",
      "-0.0054\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.0990\n",
      " 0.0525\n",
      " 0.0060\n",
      "-0.0452\n",
      "-0.0545\n",
      " 0.0623\n",
      "-0.1749\n",
      "-0.0228\n",
      "-0.0824\n",
      " 0.1469\n",
      "-0.0592\n",
      "-0.2177\n",
      " 0.0139\n",
      "-0.0996\n",
      " 0.1075\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2085\n",
      " 0.1970\n",
      " 0.1854\n",
      " 0.1727\n",
      " 0.1703\n",
      " 0.1994\n",
      " 0.1404\n",
      " 0.1782\n",
      " 0.1634\n",
      " 0.2205\n",
      " 0.1692\n",
      " 0.1297\n",
      " 0.1874\n",
      " 0.1591\n",
      " 0.2106\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [15/60], Loss: 5.1757\n",
      "Variable containing:\n",
      " 0.3086\n",
      " 0.3289\n",
      " 0.3493\n",
      " 0.3717\n",
      " 0.3757\n",
      " 0.3246\n",
      " 0.4284\n",
      " 0.3619\n",
      " 0.3879\n",
      " 0.2876\n",
      " 0.3778\n",
      " 0.4471\n",
      " 0.3458\n",
      " 0.3955\n",
      " 0.3049\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.4000\n",
      " 0.4495\n",
      " 0.4990\n",
      " 0.5535\n",
      " 0.5634\n",
      " 0.4391\n",
      " 0.6916\n",
      " 0.5297\n",
      " 0.5931\n",
      " 0.3490\n",
      " 0.5684\n",
      " 0.7371\n",
      " 0.4906\n",
      " 0.6114\n",
      " 0.3910\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.4835\n",
      " 0.5597\n",
      " 0.6358\n",
      " 0.7196\n",
      " 0.7348\n",
      " 0.5436\n",
      " 0.9320\n",
      " 0.6830\n",
      " 0.7805\n",
      " 0.4051\n",
      " 0.7425\n",
      " 1.0021\n",
      " 0.6229\n",
      " 0.8086\n",
      " 0.4697\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5598\n",
      " 0.6603\n",
      " 0.7608\n",
      " 0.8713\n",
      " 0.8914\n",
      " 0.6391\n",
      " 1.1516\n",
      " 0.8231\n",
      " 0.9517\n",
      " 0.4563\n",
      " 0.9016\n",
      " 1.2441\n",
      " 0.7437\n",
      " 0.9889\n",
      " 0.5415\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.6295\n",
      " 0.7523\n",
      " 0.8750\n",
      " 1.0099\n",
      " 1.0345\n",
      " 0.7264\n",
      " 1.3523\n",
      " 0.9510\n",
      " 1.1081\n",
      " 0.5031\n",
      " 1.0470\n",
      " 1.4652\n",
      " 0.8541\n",
      " 1.1535\n",
      " 0.6072\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [20/60], Loss: 2.2134\n",
      "Variable containing:\n",
      " 0.6932\n",
      " 0.8363\n",
      " 0.9793\n",
      " 1.1366\n",
      " 1.1652\n",
      " 0.8061\n",
      " 1.5356\n",
      " 1.0680\n",
      " 1.2510\n",
      " 0.5459\n",
      " 1.1798\n",
      " 1.6672\n",
      " 0.9550\n",
      " 1.3039\n",
      " 0.6672\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7514\n",
      " 0.9130\n",
      " 1.0746\n",
      " 1.2523\n",
      " 1.2846\n",
      " 0.8789\n",
      " 1.7031\n",
      " 1.1748\n",
      " 1.3816\n",
      " 0.5850\n",
      " 1.3011\n",
      " 1.8518\n",
      " 1.0471\n",
      " 1.4414\n",
      " 0.7221\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.8046\n",
      " 0.9831\n",
      " 1.1617\n",
      " 1.3580\n",
      " 1.3938\n",
      " 0.9455\n",
      " 1.8561\n",
      " 1.2724\n",
      " 1.5009\n",
      " 0.6207\n",
      " 1.4119\n",
      " 2.0204\n",
      " 1.1313\n",
      " 1.5669\n",
      " 0.7722\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.8532\n",
      " 1.0472\n",
      " 1.2412\n",
      " 1.4546\n",
      " 1.4934\n",
      " 1.0063\n",
      " 1.9959\n",
      " 1.3615\n",
      " 1.6098\n",
      " 0.6534\n",
      " 1.5132\n",
      " 2.1744\n",
      " 1.2082\n",
      " 1.6816\n",
      " 0.8179\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.8976\n",
      " 1.1058\n",
      " 1.3139\n",
      " 1.5429\n",
      " 1.5845\n",
      " 1.0619\n",
      " 2.1236\n",
      " 1.4430\n",
      " 1.7094\n",
      " 0.6832\n",
      " 1.6057\n",
      " 2.3151\n",
      " 1.2785\n",
      " 1.7864\n",
      " 0.8598\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [25/60], Loss: 1.0133\n",
      "Variable containing:\n",
      " 0.9382\n",
      " 1.1593\n",
      " 1.3803\n",
      " 1.6235\n",
      " 1.6677\n",
      " 1.1126\n",
      " 2.2403\n",
      " 1.5174\n",
      " 1.8004\n",
      " 0.7105\n",
      " 1.6902\n",
      " 2.4437\n",
      " 1.3427\n",
      " 1.8822\n",
      " 0.8980\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9752\n",
      " 1.2081\n",
      " 1.4410\n",
      " 1.6972\n",
      " 1.7437\n",
      " 1.1590\n",
      " 2.3469\n",
      " 1.5854\n",
      " 1.8835\n",
      " 0.7354\n",
      " 1.7674\n",
      " 2.5611\n",
      " 1.4014\n",
      " 1.9696\n",
      " 0.9329\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.0091\n",
      " 1.2528\n",
      " 1.4964\n",
      " 1.7644\n",
      " 1.8132\n",
      " 1.2014\n",
      " 2.4442\n",
      " 1.6475\n",
      " 1.9594\n",
      " 0.7582\n",
      " 1.8380\n",
      " 2.6684\n",
      " 1.4550\n",
      " 2.0495\n",
      " 0.9648\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.0401\n",
      " 1.2936\n",
      " 1.5471\n",
      " 1.8259\n",
      " 1.8766\n",
      " 1.2401\n",
      " 2.5332\n",
      " 1.7042\n",
      " 2.0287\n",
      " 0.7790\n",
      " 1.9024\n",
      " 2.7664\n",
      " 1.5040\n",
      " 2.1225\n",
      " 0.9940\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.0683\n",
      " 1.3308\n",
      " 1.5933\n",
      " 1.8821\n",
      " 1.9346\n",
      " 1.2755\n",
      " 2.6145\n",
      " 1.7561\n",
      " 2.0921\n",
      " 0.7980\n",
      " 1.9613\n",
      " 2.8560\n",
      " 1.5487\n",
      " 2.1892\n",
      " 1.0206\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [30/60], Loss: 0.5270\n",
      "Variable containing:\n",
      " 1.0942\n",
      " 1.3649\n",
      " 1.6356\n",
      " 1.9334\n",
      " 1.9876\n",
      " 1.3078\n",
      " 2.6887\n",
      " 1.8035\n",
      " 2.1500\n",
      " 0.8154\n",
      " 2.0151\n",
      " 2.9378\n",
      " 1.5896\n",
      " 2.2501\n",
      " 1.0450\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1178\n",
      " 1.3960\n",
      " 1.6742\n",
      " 1.9803\n",
      " 2.0359\n",
      " 1.3373\n",
      " 2.7565\n",
      " 1.8467\n",
      " 2.2029\n",
      " 0.8312\n",
      " 2.0643\n",
      " 3.0125\n",
      " 1.6270\n",
      " 2.3058\n",
      " 1.0672\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1394\n",
      " 1.4245\n",
      " 1.7095\n",
      " 2.0231\n",
      " 2.0801\n",
      " 1.3643\n",
      " 2.8185\n",
      " 1.8863\n",
      " 2.2512\n",
      " 0.8458\n",
      " 2.1092\n",
      " 3.0808\n",
      " 1.6611\n",
      " 2.3567\n",
      " 1.0876\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1591\n",
      " 1.4504\n",
      " 1.7418\n",
      " 2.0623\n",
      " 2.1205\n",
      " 1.3890\n",
      " 2.8751\n",
      " 1.9224\n",
      " 2.2953\n",
      " 0.8590\n",
      " 2.1502\n",
      " 3.1431\n",
      " 1.6923\n",
      " 2.4031\n",
      " 1.1061\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1771\n",
      " 1.4742\n",
      " 1.7712\n",
      " 2.0980\n",
      " 2.1574\n",
      " 1.4115\n",
      " 2.9268\n",
      " 1.9554\n",
      " 2.3357\n",
      " 0.8712\n",
      " 2.1877\n",
      " 3.2001\n",
      " 1.7207\n",
      " 2.4456\n",
      " 1.1231\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [35/60], Loss: 0.3300\n",
      "Variable containing:\n",
      " 1.1936\n",
      " 1.4959\n",
      " 1.7982\n",
      " 2.1307\n",
      " 2.1911\n",
      " 1.4321\n",
      " 2.9740\n",
      " 1.9856\n",
      " 2.3725\n",
      " 0.8823\n",
      " 2.2219\n",
      " 3.2521\n",
      " 1.7468\n",
      " 2.4843\n",
      " 1.1386\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2087\n",
      " 1.5157\n",
      " 1.8228\n",
      " 2.1605\n",
      " 2.2219\n",
      " 1.4509\n",
      " 3.0172\n",
      " 2.0131\n",
      " 2.4061\n",
      " 0.8924\n",
      " 2.2532\n",
      " 3.2997\n",
      " 1.7706\n",
      " 2.5198\n",
      " 1.1528\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2224\n",
      " 1.5338\n",
      " 1.8452\n",
      " 2.1878\n",
      " 2.2501\n",
      " 1.4681\n",
      " 3.0566\n",
      " 2.0383\n",
      " 2.4369\n",
      " 0.9017\n",
      " 2.2818\n",
      " 3.3431\n",
      " 1.7923\n",
      " 2.5521\n",
      " 1.1658\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2350\n",
      " 1.5504\n",
      " 1.8658\n",
      " 2.2127\n",
      " 2.2758\n",
      " 1.4839\n",
      " 3.0926\n",
      " 2.0613\n",
      " 2.4650\n",
      " 0.9101\n",
      " 2.3079\n",
      " 3.3828\n",
      " 1.8121\n",
      " 2.5817\n",
      " 1.1776\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2465\n",
      " 1.5655\n",
      " 1.8845\n",
      " 2.2354\n",
      " 2.2992\n",
      " 1.4982\n",
      " 3.1255\n",
      " 2.0823\n",
      " 2.4907\n",
      " 0.9179\n",
      " 2.3317\n",
      " 3.4190\n",
      " 1.8303\n",
      " 2.6087\n",
      " 1.1885\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [40/60], Loss: 0.2502\n",
      "Variable containing:\n",
      " 1.2570\n",
      " 1.5793\n",
      " 1.9017\n",
      " 2.2562\n",
      " 2.3207\n",
      " 1.5113\n",
      " 3.1555\n",
      " 2.1015\n",
      " 2.5141\n",
      " 0.9250\n",
      " 2.3535\n",
      " 3.4521\n",
      " 1.8469\n",
      " 2.6334\n",
      " 1.1984\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2666\n",
      " 1.5920\n",
      " 1.9173\n",
      " 2.2752\n",
      " 2.3403\n",
      " 1.5233\n",
      " 3.1830\n",
      " 2.1190\n",
      " 2.5355\n",
      " 0.9315\n",
      " 2.3734\n",
      " 3.4823\n",
      " 1.8620\n",
      " 2.6559\n",
      " 1.2074\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2754\n",
      " 1.6035\n",
      " 1.9316\n",
      " 2.2926\n",
      " 2.3582\n",
      " 1.5343\n",
      " 3.2081\n",
      " 2.1351\n",
      " 2.5551\n",
      " 0.9374\n",
      " 2.3916\n",
      " 3.5099\n",
      " 1.8759\n",
      " 2.6765\n",
      " 1.2157\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2834\n",
      " 1.6141\n",
      " 1.9447\n",
      " 2.3084\n",
      " 2.3746\n",
      " 1.5443\n",
      " 3.2310\n",
      " 2.1497\n",
      " 2.5730\n",
      " 0.9428\n",
      " 2.4082\n",
      " 3.5352\n",
      " 1.8885\n",
      " 2.6953\n",
      " 1.2233\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2907\n",
      " 1.6237\n",
      " 1.9567\n",
      " 2.3229\n",
      " 2.3895\n",
      " 1.5535\n",
      " 3.2519\n",
      " 2.1631\n",
      " 2.5893\n",
      " 0.9478\n",
      " 2.4234\n",
      " 3.5582\n",
      " 1.9001\n",
      " 2.7125\n",
      " 1.2302\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [45/60], Loss: 0.2178\n",
      "Variable containing:\n",
      " 1.2974\n",
      " 1.6325\n",
      " 1.9676\n",
      " 2.3362\n",
      " 2.4032\n",
      " 1.5618\n",
      " 3.2710\n",
      " 2.1753\n",
      " 2.6042\n",
      " 0.9523\n",
      " 2.4373\n",
      " 3.5793\n",
      " 1.9106\n",
      " 2.7282\n",
      " 1.2365\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3036\n",
      " 1.6406\n",
      " 1.9776\n",
      " 2.3482\n",
      " 2.4156\n",
      " 1.5695\n",
      " 3.2884\n",
      " 2.1865\n",
      " 2.6178\n",
      " 0.9565\n",
      " 2.4500\n",
      " 3.5985\n",
      " 1.9203\n",
      " 2.7425\n",
      " 1.2423\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3092\n",
      " 1.6479\n",
      " 1.9867\n",
      " 2.3593\n",
      " 2.4270\n",
      " 1.5765\n",
      " 3.3044\n",
      " 2.1967\n",
      " 2.6303\n",
      " 0.9603\n",
      " 2.4615\n",
      " 3.6160\n",
      " 1.9291\n",
      " 2.7556\n",
      " 1.2476\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3143\n",
      " 1.6547\n",
      " 1.9950\n",
      " 2.3694\n",
      " 2.4375\n",
      " 1.5829\n",
      " 3.3190\n",
      " 2.2060\n",
      " 2.6417\n",
      " 0.9638\n",
      " 2.4721\n",
      " 3.6321\n",
      " 1.9371\n",
      " 2.7676\n",
      " 1.2524\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3190\n",
      " 1.6608\n",
      " 2.0026\n",
      " 2.3786\n",
      " 2.4470\n",
      " 1.5887\n",
      " 3.3322\n",
      " 2.2145\n",
      " 2.6521\n",
      " 0.9670\n",
      " 2.4818\n",
      " 3.6467\n",
      " 1.9445\n",
      " 2.7785\n",
      " 1.2569\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [50/60], Loss: 0.2046\n",
      "Variable containing:\n",
      " 1.3233\n",
      " 1.6664\n",
      " 2.0096\n",
      " 2.3870\n",
      " 2.4557\n",
      " 1.5941\n",
      " 3.3444\n",
      " 2.2223\n",
      " 2.6615\n",
      " 0.9699\n",
      " 2.4906\n",
      " 3.6601\n",
      " 1.9512\n",
      " 2.7885\n",
      " 1.2609\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3272\n",
      " 1.6716\n",
      " 2.0159\n",
      " 2.3947\n",
      " 2.4636\n",
      " 1.5990\n",
      " 3.3555\n",
      " 2.2294\n",
      " 2.6702\n",
      " 0.9725\n",
      " 2.4987\n",
      " 3.6723\n",
      " 1.9574\n",
      " 2.7976\n",
      " 1.2646\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3308\n",
      " 1.6763\n",
      " 2.0218\n",
      " 2.4018\n",
      " 2.4709\n",
      " 1.6034\n",
      " 3.3656\n",
      " 2.2359\n",
      " 2.6781\n",
      " 0.9750\n",
      " 2.5060\n",
      " 3.6834\n",
      " 1.9630\n",
      " 2.8060\n",
      " 1.2680\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3341\n",
      " 1.6806\n",
      " 2.0271\n",
      " 2.4082\n",
      " 2.4775\n",
      " 1.6075\n",
      " 3.3749\n",
      " 2.2419\n",
      " 2.6854\n",
      " 0.9772\n",
      " 2.5128\n",
      " 3.6936\n",
      " 1.9682\n",
      " 2.8136\n",
      " 1.2711\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3371\n",
      " 1.6845\n",
      " 2.0319\n",
      " 2.4141\n",
      " 2.4835\n",
      " 1.6113\n",
      " 3.3833\n",
      " 2.2473\n",
      " 2.6920\n",
      " 0.9793\n",
      " 2.5189\n",
      " 3.7029\n",
      " 1.9729\n",
      " 2.8205\n",
      " 1.2740\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [55/60], Loss: 0.1992\n",
      "Variable containing:\n",
      " 1.3399\n",
      " 1.6881\n",
      " 2.0364\n",
      " 2.4194\n",
      " 2.4891\n",
      " 1.6147\n",
      " 3.3910\n",
      " 2.2523\n",
      " 2.6980\n",
      " 0.9812\n",
      " 2.5245\n",
      " 3.7114\n",
      " 1.9772\n",
      " 2.8269\n",
      " 1.2766\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3424\n",
      " 1.6914\n",
      " 2.0404\n",
      " 2.4243\n",
      " 2.4941\n",
      " 1.6178\n",
      " 3.3981\n",
      " 2.2568\n",
      " 2.7035\n",
      " 0.9829\n",
      " 2.5297\n",
      " 3.7192\n",
      " 1.9811\n",
      " 2.8327\n",
      " 1.2789\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3447\n",
      " 1.6944\n",
      " 2.0441\n",
      " 2.4288\n",
      " 2.4987\n",
      " 1.6207\n",
      " 3.4045\n",
      " 2.2609\n",
      " 2.7086\n",
      " 0.9845\n",
      " 2.5344\n",
      " 3.7262\n",
      " 1.9847\n",
      " 2.8380\n",
      " 1.2811\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3468\n",
      " 1.6972\n",
      " 2.0475\n",
      " 2.4329\n",
      " 2.5030\n",
      " 1.6233\n",
      " 3.4104\n",
      " 2.2647\n",
      " 2.7132\n",
      " 0.9860\n",
      " 2.5386\n",
      " 3.7327\n",
      " 1.9880\n",
      " 2.8428\n",
      " 1.2831\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3488\n",
      " 1.6997\n",
      " 2.0506\n",
      " 2.4366\n",
      " 2.5068\n",
      " 1.6257\n",
      " 3.4157\n",
      " 2.2682\n",
      " 2.7174\n",
      " 0.9873\n",
      " 2.5426\n",
      " 3.7386\n",
      " 1.9910\n",
      " 2.8472\n",
      " 1.2850\n",
      "[torch.FloatTensor of size 15x1]\n",
      "\n",
      "Epoch [60/60], Loss: 0.1970\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a8de75da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy Dataset \n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear Regression Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the Model \n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    targets = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()  \n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [%d/%d], Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, loss.data[0]))\n",
    "        \n",
    "# Plot the graph\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F.softmax: Variable containing:\n",
      " 0.1567\n",
      " 0.3116\n",
      " 0.2428\n",
      " 0.1543\n",
      " 0.1345\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "softmax: Variable containing:\n",
      " 0.1567\n",
      " 0.3116\n",
      " 0.2428\n",
      " 0.1543\n",
      " 0.1345\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-1.8533\n",
      "-1.1659\n",
      "-1.4155\n",
      "-1.8688\n",
      "-2.0059\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      "-1.8533\n",
      "-1.1659\n",
      "-1.4155\n",
      "-1.8688\n",
      "-2.0059\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      "-8.3095\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum()\n",
    "def log_softmax(x):\n",
    "#     return torch.log(softmax(x))\n",
    "    return x - torch.log(torch.exp(x[0])+torch.exp(x[1])+torch.exp(x[2])+torch.exp(x[3])+torch.exp(x[4]))\n",
    "\n",
    "data = Variable(torch.randn(5))\n",
    "# print(data)\n",
    "print('F.softmax:', F.softmax(data))\n",
    "print('softmax:', softmax(data))\n",
    "print(F.softmax(data).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data))  # theres also log_softmax\n",
    "print(log_softmax(data))  # theres also log_softmax\n",
    "print(log_softmax(data).sum())  # theres also log_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20])\n",
      "torch.Size([3, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "# import torch,ipdb\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestLSTM (\n",
      "  (rnn): LSTM(28, 50)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10 x 28], m2: [4 x 80] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1293",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-6237bd444f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [10 x 28], m2: [4 x 80] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1293"
     ]
    }
   ],
   "source": [
    "class TestLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(TestLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.rnn(x)\n",
    "        return out\n",
    "\n",
    "bs = 10\n",
    "seq_len = 7\n",
    "input_size = 28\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "\n",
    "test_lstm = TestLSTM(input_size, hidden_size, num_layers)\n",
    "print(test_lstm)\n",
    "\n",
    "input = Variable(torch.randn(seq_len, bs, input_size))\n",
    "h0 = Variable(torch.randn(num_layers, bs, hidden_size))\n",
    "c0 = Variable(torch.randn(num_layers, bs, hidden_size))\n",
    "output, h = rnn(input, (h0, c0))\n",
    "print('output', output.size())\n",
    "print('h', h.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.3207 -0.1388 -0.1382  0.8809\n",
       "-0.8027  0.1501 -0.5702  0.4479\n",
       " 0.3126 -1.0609  1.0151  1.0373\n",
       " 0.2802 -1.6143 -0.5597  1.9454\n",
       "[torch.FloatTensor of size 4x4]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
